{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ac7bbb-2a99-4502-a494-c54e51dc1c09",
   "metadata": {},
   "source": [
    "# Custom Environments and Wrappers in Gymnasium\n",
    "\n",
    "Before I can build and run a version of the Embodied Communication Game, I must learn how to build custom environments in Gymnasium. Here I'm going to run through a few tutorials on building custom `Envs` and modifying them with custom `wrappers`.\n",
    "\n",
    "## Designing Custom Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a963b5-9b41-4420-9008-32879872169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f34a9a-dc3b-4922-8971-13f38a033c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment GridWorld-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "\n",
    "    #Initializes the environment with specific attributes including size, observation_space, action_space, and any other variables \n",
    "    # defining the agent, environment, or reward structure.\n",
    "    def __init__(self, size: int = 5):\n",
    "        # The size of the square grid\n",
    "        self.size = size\n",
    "\n",
    "        # Define the agent and target location; randomly chosen in `reset` and updated in `step`\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "    #A common design pattern is to include a _get_obs method for translating state into an observation. However, this helper method\n",
    "    # is not mandatory, and you might want to compute observations directly in env.reset and env.step, which may be preferable if \n",
    "    # you want to compute them differently in each method call.\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    \n",
    "    #A similar pattern, _get_info can be used to return auxiliary information. In this Env, we would like to calculate and return\n",
    "    # Manhattan distance from the agent to the target square.\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "    #Reset is called to initiate a new episode for an environment and has two parameters, seed and options. Seed initializes the\n",
    "    # random number generator to allow us to consistently generate the same environment when there are random variables involved.\n",
    "    # Options is a dict containing any additional parameters we might want to specify during the reset.\\\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "         #We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        #Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0,self.size,size=2,dtype=int)\n",
    "        #Sample random target locations until they do not coincide with the agent's starting location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location,self.agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0,self.size,size=2,dtype=int\n",
    "            )\n",
    "\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation,info\n",
    "    \n",
    "    def step(self, action):\n",
    "        #Map the action (element of {0,1,2,3}) to a direction on the map, using our helper dictionary\n",
    "        direction = self._action_to_direction[action]\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1)\n",
    "        \n",
    "        #We use `np.clip` to make sure we don't leave the grid bound\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        truncated = False\n",
    "        reward = 1 if terminated else 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "#Now that we've defined the environment in it's own class, we can register it with gymnasium under a particular namespace\n",
    "# which we can then call gym.make() on to instantiate this custom environment.\n",
    "gym.register(id=\"GridWorld-v0\",\n",
    "             entry_point = GridWorldEnv)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ed8a81-6d1d-4b41-b8f3-07e29169487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvChecker<GridWorldEnv<GridWorld-v0>>>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating the registered version of our custom environment using gym.make().\n",
    "gym.make(\"GridWorld-v0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003b9f1-821d-42df-9433-16e443e0c457",
   "metadata": {},
   "source": [
    "# Designing my own Custom Environment\n",
    "\n",
    "### Simplified single-player color-matching game\n",
    "\n",
    "Now I'll design my own custom environment in grid world. It will have elements of the ECG, but be designed to be solvable by a single player. In this environment, the agent will have to travel to a square whose color matches another given color. The squares will be inside of a 4x4 grid and the colors and starting position of the agent will be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75aa161b-f508-4193-91fb-cf0c5b293720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleColorGame(gym.Env):\n",
    "    #Initializes the Env, including observation space and action space. This one initializes the Observation space as a grid\n",
    "    # of boxes with colors assigned to them, and the action space as the movement of the agent along the grid.\n",
    "    def __init__(self,size=2,step_limit=200):\n",
    "        #The size of one side of the square grid. It will be NxN squares in area, where N is self.size\n",
    "        self.size = size\n",
    "        self._num_colors = size**2\n",
    "\n",
    "        #This is a time limit on the number of steps the agent is allowed to take in the game. This is necessary to\n",
    "        # prevent the game from running forever if the agent's policy prevents it from moving or reaching the target.\n",
    "        self._step_limit = step_limit\n",
    "        #Integer to keep track of the number of steps taken in a particular iteration of the game\n",
    "        self._step_count = 0\n",
    "\n",
    "        #The agent location is stored inside of a local variable.\n",
    "        self._agent_location = np.array([-1,-1], dtype=np.int32)\n",
    "\n",
    "        #The colors of the boxes are also stored in a local variable. These colors are randomized on start-up. For this\n",
    "        # version of the game, I will substitute integer values for colors.\n",
    "        self._square_colors = np.arange(self._num_colors).reshape(size,size)\n",
    "\n",
    "        #The target color will be a random number between 1 and 4. This number will be initialized during the reset() method.\n",
    "        self._target_color = np.random.randint(0,self._num_colors)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent location\": gym.spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "                \"square colors\": gym.spaces.Box(0, self._num_colors-1, shape=(size,size), dtype=int),\n",
    "                \"target color\": gym.spaces.Discrete(self._num_colors)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        \n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "\n",
    "    #Helper method used to get the observation from the state, useful in reset and step methods. This version returns\n",
    "    # the properties of agent location, square colors, and the target color.\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "                \"agent location\": self._agent_location,\n",
    "                \"square colors\": self._square_colors,\n",
    "                \"target color\": self._target_color\n",
    "        }\n",
    "\n",
    "    #Helper method used to get auxiliary information from the state. Currently returns nothing.\n",
    "    def _get_info(self):\n",
    "        info = { \"info\": None }\n",
    "        return info\n",
    "\n",
    "    #Helper method for calculating the reward from the state. This will be useful as I can override it in child classes.\n",
    "    def _get_reward(self):\n",
    "        reward = 1 if (self._square_colors[tuple(self._agent_location)] == self._target_color) else 0\n",
    "        return reward\n",
    "\n",
    "    \n",
    "    #Reset the environment to an initial configuration. The initial state may involve some randomness, so the seed argument\n",
    "    # is used to guarantee an identical initial state whenever reset() is called with that seed. Options is a dict containing\n",
    "    # any additional parameters we might want to specify during the reset.\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \n",
    "        #Firstly, we will call this method to seed self.np_random with the seed argument if given.\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        #Reset the step count to 0 for the new iteration of the game\n",
    "        self._step_count = 0\n",
    "\n",
    "        #Now randomly generate a starting location for the agent using self.np_random. We generate an array of size two\n",
    "        # representing the agent's starting coordinates.\n",
    "        self._agent_location = self.np_random.integers(0,self.size,size=2)\n",
    "\n",
    "        #Generate a random permutation of the square colors, and reshape them into a sizeXsize grid.\n",
    "        self._square_colors = self.np_random.permutation(self._num_colors).reshape(self.size,self.size)\n",
    "\n",
    "        #Now we generate the target color, which is a random integer from 0 to self._num_colors inclusive.\n",
    "        self._target_color = self.np_random.integers(0,self._num_colors)\n",
    "\n",
    "        #Now we can return the observation and auxiliary info\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    #Takes an action as input and updates the state of the Env according to that Action. Step then returns an observation\n",
    "    # containing the new Env state, as well as some other additional variables and info.\n",
    "    def step(self, action):\n",
    "        #First, iterate the step count by one\n",
    "        self._step_count += 1\n",
    "        \n",
    "        #Next, we convert our action to a direction.\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        #Then we add the direction coordinates to the agend coordinates to get the new agent location. We must clip the\n",
    "        # agent location at the Box boundary, so the agent's coordinates are within 0 and self.size-1.\n",
    "        self._agent_location = np.clip(self._agent_location + direction,0,self.size-1)\n",
    "        \n",
    "        #Now we terminate the game and give the agent a reward if the square it's standing on is the target color.\n",
    "        terminated = (self._square_colors[tuple(self._agent_location)] == self._target_color)\n",
    "        \n",
    "        #We also truncate the game if self._step_count > self._step_limit.\n",
    "        truncated = (self._step_count > self._step_limit)\n",
    "        \n",
    "        #Reward is 1 if we are on the target color square, otherwise 0\n",
    "        reward = self._get_reward()\n",
    "\n",
    "        #Finally, use the helper functions to generate Obs and Info.\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "#Now let's register this environment with a namespace and try calling gym.make on it later\n",
    "gym.register(id=\"SimpleColorGame-v0\",\n",
    "            entry_point = SimpleColorGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5927e473-ec41-40bc-9e02-61ff32cbe174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see if the registration worked! We ran into a few errors with incorrect method/variable names, but after fixing those it appears this\n",
    "# game was able to load!\n",
    "env = gym.make(\"SimpleColorGame-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66688d79-e52a-4bce-9625-801f53c93f94",
   "metadata": {},
   "source": [
    "## Learning in the Custom Env with Stable-Baselines\n",
    "\n",
    "Now that I have a custom environment defined, I can try unleashing the Stable-Baselines RL algorithms on it to see if I can train a model to succeed. Of course, I have no idea currently if the environment is working properly, so I'll have to find out through trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "caf37fce-20d5-452f-8676-11cc0017857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "from stable_baselines3 import A2C, PPO, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f84cf3a5-96ea-4a51-855e-4dd3c6a5c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward of A2C Model, Simple Color Game, NxN Grid, 10000 training timesteps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: Pre-training  [0.18 +/- 0.38], Post-training [0.67 +/- 0.47]\n",
      "3x3: Pre-training  [0.20 +/- 0.40], Post-training [0.17 +/- 0.38]\n",
      "4x4: Pre-training  [0.15 +/- 0.36], Post-training [0.12 +/- 0.32]\n",
      "5x5: Pre-training  [0.05 +/- 0.22], Post-training [0.07 +/- 0.26]\n",
      "6x6: Pre-training  [0.03 +/- 0.17], Post-training [0.12 +/- 0.32]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Reward of A2C Model, Simple Color Game, NxN Grid, 10000 training timesteps\\n\")\n",
    "for i in range(2,7):\n",
    "    env = gym.make(\"SimpleColorGame-v0\",size=i,step_limit=(i**3))\n",
    "    model_A2C = A2C(\"MultiInputPolicy\", env, verbose = 0)\n",
    "    \n",
    "    mean_reward_pre, std_reward_pre = evaluate_policy(model_A2C, env, n_eval_episodes = 100) #Pre-training evaluation     \n",
    "    model_A2C.learn(total_timesteps=10000)#training the model\n",
    "    mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes = 100) #Post-training evaluation\n",
    "    \n",
    "    print(f\"{i}x{i}: Pre-training  [{mean_reward_pre:.2f} +/- {std_reward_pre:.2f}], Post-training [{mean_reward:.2f} +/- {std_reward:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79023ac-fe0b-4c56-af06-1dcc541a6de6",
   "metadata": {},
   "source": [
    "## A2C learned the simple color game!\n",
    "\n",
    "It took some finagling, but the A2C model now works with my custom color game! I had to make one major modification to the Env to allow the model to learn the game: I added a time-limit, *self._step_limit*, which sets **Truncated** variable to *True* to unceremoniously end the game if the agent takes more than *N* steps without reaching the target square. I had to do this because the untrained A2C algorithm was deciding to stubbornly stay in the corner and not move at all, causing the game to last forever.\n",
    "\n",
    "However, it looks like the model may just be learning to wander around the environment as much as possible to stumble upon the square of the correct color. I want it to use the color grid and the target color to reach the target square as fast as possible.\n",
    "\n",
    "### Time-Discounting The Reward\n",
    "\n",
    "Adding a time discounter to the reward the agent recieves should incentivise the model to be as fast as possible in reaching the target square. The simple way to do this is to decrement the reward by 1 for every step taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d586d6d-f0c7-4e1f-97b7-ebd580bfcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'll make this class extend the SimpleColorGame class. We just want to modify the reward function to subtract 0.1 for each step taken.\n",
    "class TimedColorGame(SimpleColorGame):\n",
    "    #Override the _get_reward() method to subtract 1/grid_size from the normal reward. This creates an incentive to reach the target\n",
    "    # color square quickly.\n",
    "    def _get_reward(self):\n",
    "        return super()._get_reward() - (1.0/self._num_colors)\n",
    "\n",
    "gym.register(id=\"TimedColorGame-v0\",\n",
    "             entry_point = TimedColorGame)\n",
    "env = gym.make(\"TimedColorGame-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7985aa39-5150-4f6b-ae2a-251ec36f9130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -38.01 +/- 21.78\n",
      "mean reward: 0.37 +/- 0.28\n"
     ]
    }
   ],
   "source": [
    "model_A2C = A2C(\"MultiInputPolicy\", env, verbose = 0)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes = 100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "      \n",
    "model_A2C.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes = 100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1838cd-f622-413f-8437-9691dba9c82d",
   "metadata": {},
   "source": [
    "## Is the A2C algorithm finding the correct square efficiently?\n",
    "\n",
    "It's hard to tell if the A2C algorithm is finding the optimal path to the square of the target color, or if it's just moving around the tiny 2x2 grid. Let's instantiate some larger versions of this game to see how quickly the model is approaching the target square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ec7f6c6-99db-46a4-892e-a4c7aa7dead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rewards for Timed Color Game with NxN grid-size, using A2C, 10000 training timesteps\n",
      "2x2: Pre-training  [-1.62 +/- 1.22], Post-training [0.37 +/- 0.27]\n",
      "3x3: Pre-training  [-2.31 +/- 1.60], Post-training [-2.36 +/- 1.56]\n",
      "4x4: Pre-training  [-3.47 +/- 1.61], Post-training [-3.37 +/- 1.72]\n",
      "5x5: Pre-training  [-4.50 +/- 1.71], Post-training [-4.20 +/- 2.07]\n",
      "6x6: Pre-training  [-5.61 +/- 1.66], Post-training [-5.82 +/- 1.19]\n"
     ]
    }
   ],
   "source": [
    "#We'll run the A2C algorithm for 5 instances of the game, from a 2x2 to a 6x6. Performance before and after learning\n",
    "# will be compared for each game size.\n",
    "print(\"Mean Rewards for Timed Color Game with NxN grid-size, using A2C, 10000 training timesteps\")\n",
    "for i in range(2,7):\n",
    "    #Instantiate the TimedColorGame with size=i and step_limit = i^3, so any improvement in the trained model over\n",
    "    # an untrained model should be apparent. Previously even the trained models appeared to be running into the step\n",
    "    # limit.\n",
    "    env = gym.make(\"TimedColorGame-v0\",size=i,step_limit=(i**3))\n",
    "    model_A2C = A2C(\"MultiInputPolicy\", env, verbose = 0)\n",
    "    \n",
    "    mean_reward_pre, std_reward_pre = evaluate_policy(model_A2C, env, n_eval_episodes = 100) #Pre-training evaluation     \n",
    "    model_A2C.learn(total_timesteps=10000)#training the model\n",
    "    mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes = 100) #Post-training evaluation\n",
    "    print(f\"{i}x{i}: Pre-training  [{mean_reward_pre:.2f} +/- {std_reward_pre:.2f}], Post-training [{mean_reward:.2f} +/- {std_reward:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9a50b-a2f3-4d9b-8283-bb8bc68e322f",
   "metadata": {},
   "source": [
    "### After testing the model repeatedly, it appears that it is not finding the correct square efficiently.\n",
    "\n",
    "The model only shows significant improvement in the 2x2 square, which I assume is because it's just rnadomly walking around the edge until it stumbles upon the square of the correct color. I'll have to try modifying the algorithm or the game to more efficiently find the right square.\n",
    "\n",
    "### Let's try adjusting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00d27a-e4a1-4a20-b0db-2b9a320459aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3x3: Pre-training  [0.12 +/- 0.32], Post-training [0.14 +/- 0.35]\n",
      "3x3: Pre-training  [0.12 +/- 0.32], Post-training [0.19 +/- 0.39]\n"
     ]
    }
   ],
   "source": [
    "#I'm going to try out different models and training paradigms on a 3x3 grid to see which, if any, of them can show\n",
    "# improvement on the simple color game. I'm now realizing that these models come with a pre-programmed discount\n",
    "# factor, so adding a time penalty to the reward structure is probably unnecessary and counterproductive at worst.\n",
    "env = gym.make(\"SimpleColorGame-v0\",size=3)\n",
    "PPO = PPO(\"MultiInputPolicy\",env,verbose=0)\n",
    "DQN = DQN(\"MultiInputPolicy\",env,verbose=0)\n",
    "A2C = A2C(\"MultiInputPolicy\",env,verbose=0)\n",
    "\n",
    "for model in [PPO,DQN,A2C]:\n",
    "    mean_reward_pre, std_reward_pre = evaluate_policy(model, env, n_eval_episodes = 100) #Pre-training evaluation     \n",
    "    model.learn(total_timesteps=10000)#training the model\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes = 100) #Post-training evaluation\n",
    "    print(f\"3x3: Pre-training  [{mean_reward_pre:.2f} +/- {std_reward_pre:.2f}], Post-training [{mean_reward:.2f} +/- {std_reward:.2f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
