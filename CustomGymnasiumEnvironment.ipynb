{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ac7bbb-2a99-4502-a494-c54e51dc1c09",
   "metadata": {},
   "source": [
    "# Custom Environments and Wrappers in Gymnasium\n",
    "\n",
    "Before I can build and run a version of the Embodied Communication Game, I must learn how to build custom environments in Gymnasium. Here I'm going to run through a few tutorials on building custom `Envs` and modifying them with custom `wrappers`.\n",
    "\n",
    "## Designing Custom Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a963b5-9b41-4420-9008-32879872169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GYMNASIUM IMPORTS\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# SB3 ALGORITHM IMPORTS\n",
    "from stable_baselines3 import A2C, PPO, DQN, HerReplayBuffer\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f34a9a-dc3b-4922-8971-13f38a033c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "\n",
    "    # Initializes the environment with specific attributes including size, observation_space, action_space, and any other variables\n",
    "    # defining the agent, environment, or reward structure.\n",
    "    def __init__(self, size: int = 5):\n",
    "        # The size of the square grid\n",
    "        self._size = size\n",
    "\n",
    "        # Define the agent and target location; randomly chosen in `reset` and updated in `step`\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "\n",
    "    # A common design pattern is to include a _get_obs method for translating state into an observation. However, this helper method\n",
    "    # is not mandatory, and you might want to compute observations directly in env.reset and env.step, which may be preferable if\n",
    "    # you want to compute them differently in each method call.\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    # A similar pattern, _get_info can be used to return auxiliary information. In this Env, we would like to calculate and return\n",
    "    # Manhattan distance from the agent to the target square.\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    # Reset is called to initiate a new episode for an environment and has two parameters, seed and options. Seed initializes the\n",
    "    # random number generator to allow us to consistently generate the same environment when there are random variables involved.\n",
    "    # Options is a dict containing any additional parameters we might want to specify during the reset.\\\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self._size, size=2, dtype=int)\n",
    "        # Sample random target locations until they do not coincide with the agent's starting location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self.agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self._size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to a direction on the map, using our helper dictionary\n",
    "        direction = self._action_to_direction[action]\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self._size - 1\n",
    "        )\n",
    "\n",
    "        # We use `np.clip` to make sure we don't leave the grid bound\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        truncated = False\n",
    "        reward = 1 if terminated else 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# Now that we've defined the environment in it's own class, we can register it with gymnasium under a particular namespace\n",
    "# which we can then call gym.make() on to instantiate this custom environment.\n",
    "gym.register(id=\"GridWorld-v0\", entry_point=GridWorldEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ed8a81-6d1d-4b41-b8f3-07e29169487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvChecker<GridWorldEnv<GridWorld-v0>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the registered version of our custom environment using gym.make().\n",
    "gym.make(\"GridWorld-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003b9f1-821d-42df-9433-16e443e0c457",
   "metadata": {},
   "source": [
    "# Designing my own Custom Environment\n",
    "\n",
    "### Simplified single-player color-matching game\n",
    "\n",
    "Now I'll design my own custom environment in grid world. It will have elements of the ECG, but be designed to be solvable by a single player. In this environment, the agent will have to travel to a square whose color matches another given color. The squares will be inside of a 4x4 grid and the colors and starting position of the agent will be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75aa161b-f508-4193-91fb-cf0c5b293720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SimpleColorGame-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "class SimpleColorGame(gym.Env):\n",
    "    # Initializes the Env, including observation space and action space. This one initializes the Observation space as a grid\n",
    "    # of boxes with colors assigned to them, and the action space as the movement of the agent along the grid.\n",
    "    def __init__(self, size=2, step_limit=200):\n",
    "        # The size of one side of the square grid. It will be NxN squares in area, where N is self._size\n",
    "        self._size = size\n",
    "        self._num_colors = size**2\n",
    "\n",
    "        # This is a time limit on the number of steps the agent is allowed to take in the game. This is necessary to\n",
    "        # prevent the game from running forever if the agent's policy prevents it from moving or reaching the target.\n",
    "        self._step_limit = step_limit\n",
    "        # Integer to keep track of the number of steps taken in a particular iteration of the game\n",
    "        self._step_count = 0\n",
    "\n",
    "        # The agent location is stored inside of a local variable.\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # The colors of the boxes are also stored in a local variable. These colors are randomized on start-up. For this\n",
    "        # version of the game, I will substitute integer values for colors.\n",
    "        self._square_colors = np.arange(self._num_colors).reshape(size, size)\n",
    "\n",
    "        # The target color will be a random number between 1 and 4. This number will be initialized during the reset() method.\n",
    "        self._target_color = np.random.randint(0, self._num_colors)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent location\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"square colors\": gym.spaces.Box(\n",
    "                    0, self._num_colors - 1, shape=(size, size), dtype=int\n",
    "                ),\n",
    "                \"target color\": gym.spaces.Discrete(self._num_colors),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "\n",
    "    # Helper method used to get the observation from the state, useful in reset and step methods. This version returns\n",
    "    # the properties of agent location, square colors, and the target color.\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent location\": self._agent_location,\n",
    "            \"square colors\": self._square_colors,\n",
    "            \"target color\": self._target_color,\n",
    "        }\n",
    "\n",
    "    # Helper method used to get auxiliary information from the state. Currently returns nothing.\n",
    "    def _get_info(self):\n",
    "        info = {\"info\": None}\n",
    "        return info\n",
    "\n",
    "    # Helper method for calculating the reward from the state. This will be useful as I can override it in child classes.\n",
    "    def _get_reward(self):\n",
    "        reward = (\n",
    "            1\n",
    "            if (self._square_colors[tuple(self._agent_location)] == self._target_color)\n",
    "            else 0\n",
    "        )\n",
    "        return reward\n",
    "\n",
    "    # Reset the environment to an initial configuration. The initial state may involve some randomness, so the seed argument\n",
    "    # is used to guarantee an identical initial state whenever reset() is called with that seed. Options is a dict containing\n",
    "    # any additional parameters we might want to specify during the reset.\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "\n",
    "        # Firstly, we will call this method to seed self.np_random with the seed argument if given.\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset the step count to 0 for the new iteration of the game\n",
    "        self._step_count = 0\n",
    "\n",
    "        # Now randomly generate a starting location for the agent using self.np_random. We generate an array of size two\n",
    "        # representing the agent's starting coordinates.\n",
    "        self._agent_location = self.np_random.integers(0, self._size, size=2)\n",
    "\n",
    "        # Generate a random permutation of the square colors, and reshape them into a sizeXsize grid.\n",
    "        self._square_colors = self.np_random.permutation(self._num_colors).reshape(\n",
    "            self._size, self._size\n",
    "        )\n",
    "\n",
    "        # Now we generate the target color, which is a random integer from 0 to self._num_colors inclusive.\n",
    "        self._target_color = self.np_random.integers(0, self._num_colors)\n",
    "\n",
    "        # Now we can return the observation and auxiliary info\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    # Takes an action as input and updates the state of the Env according to that Action. Step then returns an observation\n",
    "    # containing the new Env state, as well as some other additional variables and info.\n",
    "    def step(self, action):\n",
    "        # First, iterate the step count by one\n",
    "        self._step_count += 1\n",
    "\n",
    "        # Next, we convert our action to a direction.\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        # Then we add the direction coordinates to the agend coordinates to get the new agent location. We must clip the\n",
    "        # agent location at the Box boundary, so the agent's coordinates are within 0 and self._size-1.\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self._size - 1\n",
    "        )\n",
    "\n",
    "        # Now we terminate the game and give the agent a reward if the square it's standing on is the target color.\n",
    "        terminated = (\n",
    "            self._square_colors[tuple(self._agent_location)] == self._target_color\n",
    "        )\n",
    "\n",
    "        # We also truncate the game if self._step_count > self._step_limit.\n",
    "        truncated = self._step_count > self._step_limit\n",
    "\n",
    "        # Reward is 1 if we are on the target color square, otherwise 0\n",
    "        reward = self._get_reward()\n",
    "\n",
    "        # Finally, use the helper functions to generate Obs and Info.\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# Now let's register this environment with a namespace and try calling gym.make on it later\n",
    "gym.register(id=\"SimpleColorGame-v0\", entry_point=SimpleColorGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5927e473-ec41-40bc-9e02-61ff32cbe174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if the registration worked! We ran into a few errors with incorrect method/variable names, but after fixing those it appears this\n",
    "# game was able to load!\n",
    "env = gym.make(\"SimpleColorGame-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66688d79-e52a-4bce-9625-801f53c93f94",
   "metadata": {},
   "source": [
    "## Learning in the Custom Env with Stable-Baselines\n",
    "\n",
    "Now that I have a custom environment defined, I can try unleashing the Stable-Baselines RL algorithms on it to see if I can train a model to succeed. Of course, I have no idea currently if the environment is working properly, so I'll have to find out through trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f84cf3a5-96ea-4a51-855e-4dd3c6a5c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward of A2C Model, Simple Color Game, NxN Grid, 10000 training timesteps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: Pre-training  [0.18 +/- 0.38], Post-training [0.67 +/- 0.47]\n",
      "3x3: Pre-training  [0.20 +/- 0.40], Post-training [0.17 +/- 0.38]\n",
      "4x4: Pre-training  [0.15 +/- 0.36], Post-training [0.12 +/- 0.32]\n",
      "5x5: Pre-training  [0.05 +/- 0.22], Post-training [0.07 +/- 0.26]\n",
      "6x6: Pre-training  [0.03 +/- 0.17], Post-training [0.12 +/- 0.32]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Mean Reward of A2C Model, Simple Color Game, NxN Grid, 10000 training timesteps\\n\"\n",
    ")\n",
    "for i in range(2, 7):\n",
    "    env = gym.make(\"SimpleColorGame-v0\", size=i, step_limit=(i**3))\n",
    "    model_A2C = A2C(\"MultiInputPolicy\", env, verbose=0)\n",
    "\n",
    "    mean_reward_pre, std_reward_pre = evaluate_policy(\n",
    "        model_A2C, env, n_eval_episodes=100\n",
    "    )  # Pre-training evaluation\n",
    "    model_A2C.learn(total_timesteps=10000)  # training the model\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model_A2C, env, n_eval_episodes=100\n",
    "    )  # Post-training evaluation\n",
    "\n",
    "    print(\n",
    "        f\"{i}x{i}: Pre-training  [{mean_reward_pre:.2f} +/- {std_reward_pre:.2f}], Post-training [{mean_reward:.2f} +/- {std_reward:.2f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79023ac-fe0b-4c56-af06-1dcc541a6de6",
   "metadata": {},
   "source": [
    "## A2C learned the simple color game!\n",
    "\n",
    "It took some finagling, but the A2C model now works with my custom color game! I had to make one major modification to the Env to allow the model to learn the game: I added a time-limit, *self._step_limit*, which sets **Truncated** variable to *True* to unceremoniously end the game if the agent takes more than *N* steps without reaching the target square. I had to do this because the untrained A2C algorithm was deciding to stubbornly stay in the corner and not move at all, causing the game to last forever.\n",
    "\n",
    "However, it looks like the model may just be learning to wander around the environment as much as possible to stumble upon the square of the correct color. I want it to use the color grid and the target color to reach the target square as fast as possible.\n",
    "\n",
    "### Time-Discounting The Reward\n",
    "\n",
    "Adding a time discounter to the reward the agent recieves should incentivise the model to be as fast as possible in reaching the target square. The simple way to do this is to decrement the reward by 1 for every step taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d586d6d-f0c7-4e1f-97b7-ebd580bfcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment TimedColorGame-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# I'll make this class extend the SimpleColorGame class. We just want to modify the reward function to subtract 0.1 for each step taken.\n",
    "class TimedColorGame(SimpleColorGame):\n",
    "    # Override the _get_reward() method to subtract 1/self._step_limit from the normal reward. This creates an incentive to reach the target\n",
    "    # square quickly.\n",
    "    def _get_reward(self):\n",
    "        reward = (\n",
    "            (1.0 - self._step_count / self._step_limit)\n",
    "            if (self._square_colors[tuple(self._agent_location)] == self._target_color)\n",
    "            else 0\n",
    "        )\n",
    "        return reward\n",
    "\n",
    "\n",
    "gym.register(id=\"TimedColorGame-v0\", entry_point=TimedColorGame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1838cd-f622-413f-8437-9691dba9c82d",
   "metadata": {},
   "source": [
    "## Is the A2C algorithm finding the correct square efficiently?\n",
    "\n",
    "It's hard to tell if the A2C algorithm is finding the optimal path to the square of the target color, or if it's just moving around the tiny 2x2 grid. Let's instantiate some larger versions of this game to see how quickly the model is approaching the target square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ec7f6c6-99db-46a4-892e-a4c7aa7dead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rewards for Timed Color Game with NxN grid-size, using A2C, 10000 training timesteps\n",
      "2x2: Pre-training  [-1.62 +/- 1.22], Post-training [0.37 +/- 0.27]\n",
      "3x3: Pre-training  [-2.31 +/- 1.60], Post-training [-2.36 +/- 1.56]\n",
      "4x4: Pre-training  [-3.47 +/- 1.61], Post-training [-3.37 +/- 1.72]\n",
      "5x5: Pre-training  [-4.50 +/- 1.71], Post-training [-4.20 +/- 2.07]\n",
      "6x6: Pre-training  [-5.61 +/- 1.66], Post-training [-5.82 +/- 1.19]\n"
     ]
    }
   ],
   "source": [
    "# We'll run the A2C algorithm for 5 instances of the game, from a 2x2 to a 6x6. Performance before and after learning\n",
    "# will be compared for each game size.\n",
    "print(\n",
    "    \"Mean Rewards for Timed Color Game with NxN grid-size, using A2C, 10000 training timesteps\"\n",
    ")\n",
    "for i in range(2, 7):\n",
    "    # Instantiate the TimedColorGame with size=i and step_limit = i^3, so any improvement in the trained model over\n",
    "    # an untrained model should be apparent. Previously even the trained models appeared to be running into the step\n",
    "    # limit.\n",
    "    env = gym.make(\"TimedColorGame-v0\", size=i, step_limit=(i**3))\n",
    "    model_A2C = A2C(\"MultiInputPolicy\", env, verbose=0)\n",
    "\n",
    "    mean_reward_pre, std_reward_pre = evaluate_policy(\n",
    "        model_A2C, env, n_eval_episodes=100\n",
    "    )  # Pre-training evaluation\n",
    "    model_A2C.learn(total_timesteps=10000)  # training the model\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model_A2C, env, n_eval_episodes=100\n",
    "    )  # Post-training evaluation\n",
    "    print(\n",
    "        f\"{i}x{i}: Pre-training  [{mean_reward_pre:.2f} +/- {std_reward_pre:.2f}], Post-training [{mean_reward:.2f} +/- {std_reward:.2f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9a50b-a2f3-4d9b-8283-bb8bc68e322f",
   "metadata": {},
   "source": [
    "### After testing the model repeatedly, it appears that it is not finding the correct square efficiently.\n",
    "\n",
    "The model only shows significant improvement in the 2x2 square, which I assume is because it's just rnadomly walking around the edge until it stumbles upon the square of the correct color. I'll have to try modifying the algorithm or the game to more efficiently find the right square.\n",
    "\n",
    "### Let's try adjusting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d00d27a-e4a1-4a20-b0db-2b9a320459aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.ppo.ppo.PPO'>\n",
      "5x5: Pre-training  [0.08 +/- 0.26], Post-training [0.09 +/- 0.28]\n",
      "<class 'stable_baselines3.dqn.dqn.DQN'>\n",
      "5x5: Pre-training  [0.09 +/- 0.28], Post-training [0.09 +/- 0.28]\n",
      "<class 'stable_baselines3.a2c.a2c.A2C'>\n",
      "5x5: Pre-training  [0.09 +/- 0.28], Post-training [0.10 +/- 0.30]\n"
     ]
    }
   ],
   "source": [
    "# I'm going to try out different models and training paradigms on a 3x3 grid to see which, if any, of them can show\n",
    "# improvement on the simple color game. I'm now realizing that these models come with a pre-programmed discount\n",
    "# factor, so adding a time penalty to the reward structure is unnecessary and may be counterproductive at worst.\n",
    "env = gym.make(\"TimedColorGame-v0\", size=5, step_limit=500)\n",
    "mPPO = PPO(\"MultiInputPolicy\", env, verbose=0)\n",
    "mDQN = DQN(\"MultiInputPolicy\", env, verbose=0)\n",
    "mA2C = A2C(\"MultiInputPolicy\", env, verbose=0)\n",
    "\n",
    "for model in [mPPO, mDQN, mA2C]:\n",
    "    print(type(model))\n",
    "    mean_reward_pre, std_reward_pre = evaluate_policy(\n",
    "        model, env, n_eval_episodes=1000\n",
    "    )  # Pre-training evaluation\n",
    "    model.learn(total_timesteps=20000)  # training the model\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, env, n_eval_episodes=1000\n",
    "    )  # Post-training evaluation\n",
    "    print(\n",
    "        f\"5x5: Pre-training  [{mean_reward_pre:.2f} +/- {std_reward_pre:.2f}], Post-training [{mean_reward:.2f} +/- {std_reward:.2f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f252e73-4723-4766-bae8-744df870b984",
   "metadata": {},
   "source": [
    "### How does model training time correlate to model performance?\n",
    "\n",
    "I expect to see an improvement in the performance of the models in the Color game given more training time. However, I somewhat naively assumed that 10,000 training timesteps (the number of training timesteps given in the SB3 training examples) would be sufficient to learn the Color Game for small grid sizes. A human player, for instance, would very quickly be able to play the game optimally, moving to the square with the target color in an average of 2 moves in the 3x3 version of the color game. The models which I've tested so far have come nowhere close to that performance within a 3x3 grid.\n",
    "\n",
    "Perhaps I was incorrect in my estimate of what would be an adequate training time, and orders of magnitude greater training time is required for the models to achieve a performance comparable to a human player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "523b8d4b-ac50-48da-9668-65cee308ce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training Timeteps mean reward: 0.22 +/- 0.40\n",
      "10,000 Training Timesteps mean reward: 0.12 +/- 0.32\n",
      "100,000 Training Timesteps mean reward: 0.34 +/- 0.44\n",
      "1,000,000 Training Timesteps mean reward: 0.59 +/- 0.39\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"TimedColorGame-v0\", size=3, step_limit=27)\n",
    "\n",
    "model_A2C = A2C(\"MultiInputPolicy\", env, verbose=0)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes=100)\n",
    "print(f\"0 Training Timeteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model_A2C.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes=100)\n",
    "print(f\"10,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model_A2C.save(\"TCG0.s3.A2C.Te4\")\n",
    "\n",
    "model_A2C.learn(total_timesteps=90000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes=100)\n",
    "print(f\"100,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model_A2C.save(\"TCG0.s3.A2C.Te5\")\n",
    "\n",
    "model_A2C.learn(total_timesteps=900000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes=100)\n",
    "print(\n",
    "    f\"1,000,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\"\n",
    ")\n",
    "\n",
    "model_A2C.save(\"TCG.s3.A2C.Te6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37724f8b-6dcb-4934-8e5b-664550b35ab2",
   "metadata": {},
   "source": [
    "## Increasing the Training Time beyond 10,000 Timesteps Significantly Improved performance!\n",
    "\n",
    "It appears that the training time was correlated with a linear increase in performance. In fact, the bot actually showed a decrease in performance after 10,000 training timesteps, and achieved a very respectable average of 0.59 points per episode after 1,000,000 training timesteps. It just goes to show that the models can have a decrease in performance after some training, and may take a very large number of training timesteps to achieve optimal performance (10,000 timesteps is orders of magnitude away from the requirement for even a simple game like this one). I'd also like to see how the PPO model performs on this env after various numbers of training timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2f7bf03-75fb-479d-905d-01b3d03d3281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training Timeteps mean reward: 0.08 +/- 0.26\n",
      "10,000 Training Timesteps mean reward: 0.17 +/- 0.36\n",
      "100,000 Training Timesteps mean reward: 0.22 +/- 0.40\n",
      "1,000,000 Training Timesteps mean reward: 0.67 +/- 0.37\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"TimedColorGame-v0\", size=3, step_limit=27)\n",
    "\n",
    "mPPO = PPO(\"MultiInputPolicy\", env, verbose=0)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mPPO, env, n_eval_episodes=100)\n",
    "print(f\"0 Training Timeteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "mPPO.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mPPO, env, n_eval_episodes=100)\n",
    "print(f\"10,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "mPPO.save(\"TCG0.s3.PPO.Te4\")\n",
    "\n",
    "mPPO.learn(total_timesteps=100000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mPPO, env, n_eval_episodes=100)\n",
    "print(f\"100,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "mPPO.save(\"TCG0.s3.PPO.Te5\")\n",
    "\n",
    "mPPO.learn(total_timesteps=1000000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mPPO, env, n_eval_episodes=100)\n",
    "print(\n",
    "    f\"1,000,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\"\n",
    ")\n",
    "\n",
    "mPPO.save(\"TCG0.s3.PPO.Te6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3db98-9c26-4d32-96cd-7cda0f917c67",
   "metadata": {},
   "source": [
    "### PPO was also successful!\n",
    "\n",
    "And it looks like this instance of PPO actually required a longer training time to achieve a similar performance in the task, as its performance after 100,000 timesteps was worse than the A2C model but its performance after 1,000,000 timesteps was slightly better! It's really interesting to see that a training inflection point occurs sometime between the 100K and 1,000,000 training timesteps. I would not have thought that it would take so many iterations for these models to learn how to complete this rather simple game.\n",
    "\n",
    "### Now let's try the DQN model for shits and giggles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcc62816-d033-46f9-88a3-a7eff3fdc117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training Timeteps mean reward: 0.16 +/- 0.35\n",
      "10,000 Training Timesteps mean reward: 0.21 +/- 0.39\n",
      "100,000 Training Timesteps mean reward: 0.25 +/- 0.42\n",
      "1,000,000 Training Timesteps mean reward: 0.19 +/- 0.38\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"TimedColorGame-v0\", size=3, step_limit=27)\n",
    "\n",
    "mDQN = DQN(\"MultiInputPolicy\", env, verbose=0)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mDQN, env, n_eval_episodes=100)\n",
    "print(f\"0 Training Timeteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "mDQN.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mDQN, env, n_eval_episodes=100)\n",
    "print(f\"10,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "mDQN.save(\"TCG0.s3.DQN.Te4\")\n",
    "\n",
    "mDQN.learn(total_timesteps=100000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mDQN, env, n_eval_episodes=100)\n",
    "print(f\"100,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "mDQN.save(\"TCG0.s3.DQN.Te5\")\n",
    "\n",
    "mDQN.learn(total_timesteps=1000000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mDQN, env, n_eval_episodes=100)\n",
    "print(\n",
    "    f\"1,000,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\"\n",
    ")\n",
    "\n",
    "mDQN.save(\"TCG0.s3.DQN.Te6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88102700-f33f-444b-bc72-e9e9d0fdc5ed",
   "metadata": {},
   "source": [
    "### The DQN Model got worse!\n",
    "\n",
    "Interestingly, the DQN model appears to have decreased in performance between 100,000 and 1,000,000 training timesteps. I'm not sure about this, but I believe that the nature of this problem makes Q-tables rather useless, since the location of the starting and target square is randomized for each trial. I suspect this is why the DQN model had such abysmal performance after 1,000,000 timesteps. This is odd, however, since even if the bot ignores the target square, simply walking through each square in succession should yield an average reward of ~0.66 or more. In fact, I'm guessing that that's what the PPO and A2C models learned to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b38c1-d6a1-4c93-891a-04c390a4d695",
   "metadata": {},
   "source": [
    "### Vectorizing Custom Environments in SB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45cf9eec-bfc7-48fc-8489-968c450a6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\envs\\registration.py:788: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='rgb_array' that is not in the possible render_modes ([]).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# I want to make a vec_env using SB3's make_vec_env util. It takes an env name as the primary argument, similar to\n",
    "# gym.make(), as well as n_envs (number of envs to vectorize into) and env_kwargs (a dict for by-keyword arguments\n",
    "# you would otherwise pass into gym.make(). VecEnvs are essentially multiple Envs stacked on top of each other,\n",
    "# allowing for more efficient training of some models. However, some models will train worse on VecEnvs.\n",
    "vec_env = make_vec_env(\n",
    "    \"TimedColorGame-v0\", n_envs=10, env_kwargs={\"size\": 2, \"step_limit\": 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b31108bf-7c5c-4597-89bf-1b91572998ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,000 Training Timesteps mean reward: 0.55 +/- 0.24\n",
      "10,000 Training Timesteps mean reward: 0.20 +/- 0.32\n",
      "10,000 Training Timesteps mean reward: 0.32 +/- 0.35\n",
      "10,000 Training Timesteps mean reward: 0.41 +/- 0.35\n",
      "10,000 Training Timesteps mean reward: 0.48 +/- 0.32\n",
      "10,000 Training Timesteps mean reward: 0.23 +/- 0.34\n",
      "10,000 Training Timesteps mean reward: 0.38 +/- 0.36\n",
      "10,000 Training Timesteps mean reward: 0.53 +/- 0.26\n",
      "10,000 Training Timesteps mean reward: 0.33 +/- 0.35\n",
      "10,000 Training Timesteps mean reward: 0.42 +/- 0.32\n",
      "100,000 Training Timesteps mean reward: 0.68 +/- 0.11\n",
      "100,000 Training Timesteps mean reward: 0.67 +/- 0.12\n",
      "100,000 Training Timesteps mean reward: 0.69 +/- 0.11\n",
      "100,000 Training Timesteps mean reward: 0.67 +/- 0.12\n",
      "100,000 Training Timesteps mean reward: 0.67 +/- 0.12\n",
      "100,000 Training Timesteps mean reward: 0.69 +/- 0.11\n",
      "100,000 Training Timesteps mean reward: 0.67 +/- 0.12\n",
      "100,000 Training Timesteps mean reward: 0.69 +/- 0.11\n",
      "100,000 Training Timesteps mean reward: 0.69 +/- 0.11\n",
      "100,000 Training Timesteps mean reward: 0.68 +/- 0.11\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    mPPO = PPO(\"MultiInputPolicy\", vec_env, verbose=0)\n",
    "    mPPO.learn(total_timesteps=10000)\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(mPPO, vec_env, n_eval_episodes=100)\n",
    "    print(\n",
    "        f\"10,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\"\n",
    "    )\n",
    "for i in range(10):\n",
    "    mPPO.learn(total_timesteps=100000)\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(mPPO, vec_env, n_eval_episodes=100)\n",
    "    print(\n",
    "        f\"100,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc78e31-b187-40c9-a437-e60ac6cafae0",
   "metadata": {},
   "source": [
    "### That is an optimal performance by the PPO model.\n",
    "\n",
    "Assuming perfect behaviour, the agent should take an average of 1.5 steps to reach the target in the 2x2 Color Game. This results in an optimal mean reward of 0.625, +/- 0.125. If the agent were just making loops around the grid, its mean number of steps would be 2.5, and its mean reward would be 0.375 +/- 0.125. This score indicates that the PPO model is immediately taking the shortest path to the square of the target color!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b6c8de0-3f4d-4ea2-8d81-623f526d6335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000,000 Training Timesteps mean reward: 0.40 +/- 0.37\n"
     ]
    }
   ],
   "source": [
    "# I'm going to train the A2C model as well just to see if it too can achieve a near-optimal performance.\n",
    "mA2C = A2C(\"MultiInputPolicy\", vec_env, verbose=0)\n",
    "\n",
    "mA2C.learn(total_timesteps=1000000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mA2C, vec_env, n_eval_episodes=100)\n",
    "print(\n",
    "    f\"1,000,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\"\n",
    ")\n",
    "\n",
    "mA2C.save(\"TCG0-s2-sl4-v6_A2C-Te6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e1c9e-671e-4a78-b09a-1898a7f9d9e5",
   "metadata": {},
   "source": [
    "### This reflects subpar performance by A2C.\n",
    "\n",
    "Let's run the PPO Model once more for a longer time control to see if we can confirm the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26684aee-ae74-4522-9732-208d28d9a403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000,000 Training Timesteps mean reward: 0.69 +/- 0.11\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(mPPO, vec_env, n_eval_episodes=10000)\n",
    "print(\n",
    "    f\"1,000,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb5cf43-7b8a-4e4f-a31c-a5bb94c30406",
   "metadata": {},
   "source": [
    "These models are still performing poorly at larger sizes, and take a long time to train on the 2x2 grid.\n",
    "\n",
    "## Hindsight Experience Replay to improve performance.\n",
    "\n",
    "I'm going to try to improve the model performance/learning time using a method known as Hindsight Experience Replay (HER). HER attempts to help the model learn about the environment even when it doesn't reach the goal state, by iteratively treating other states it reached as if they were the goal state. Essentially, it teaches the model how to reach various states in the environment other than the goal state. The model can use this information to aid in its learning how to reach the goal state. It's kind of hard to explain in an intuitive way, but there's an excellent blog which got me to understand it in an intuitive way: https://towardsdatascience.com/reinforcement-learning-with-hindsight-experience-replay-1fee5704f2f8\n",
    "\n",
    "HER is most useful for large state-goal spaces in which it's extremely improbable for an untrained model to ever reach the goal state and get rewarded, meaning it's essentially impossible for conventional reinforcement learning to take place. The larger grid sizes in the color game are approaching that concept. Additionally, if I implement a version of the game where the agent has to 'stop' on the target square, that version of the game will likely be sparse and contain few rewards.\n",
    "\n",
    "### Modifying the Color Game to fulfill GoalEnv interface.\n",
    "\n",
    "In order to implement HER, I will need to modify the Color Game to fulfill the GoalEnv interface. It must include 'achieved goal' and 'desired goal' as keywords in the 'obervation' dict, as well as a function 'compute_goal' to compute the reward gained by the model given the 'achieved goal' and 'desired goal'. To implement this, I'm going to make a modified version of the Simple Color Game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adab37-cabb-4d86-9fab-8a6fcbaf8e18",
   "metadata": {},
   "source": [
    "# sigh...\n",
    "\n",
    "Apparently, for some reason Gymnasium doesn't support nested observation spaces (Tuple/Dict inside a Tuple/Dict). I was thinking for a second that I'm screwed, but I might actually be able to save this by having the current \"observation\" key within self._get_obs() just contain the agent's current location. In fact, I could try doing away with it altogether, as the relevant information is still there under the 'achieved goal' and 'desired goal' keys. I'm banking on the fact that the actual 'observation' is whatever is returned by self._get_obs(), and not the little section labeled 'observation' within that Dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84af647f-d892-4ab0-919a-f9c4e96db568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SimpleColorGame-v1 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# This GoalEnv class will be essentially the same as the Simple Color Game, but will include the variables achieved_goal and desired_goal,\n",
    "# and a function relating them to a reward.\n",
    "class SimpleColorGameGoalEnv(SimpleColorGame):\n",
    "    # Override self.observation_space to include achieved_goal and desired_goal.\n",
    "    def __init__(self, size=2, step_limit=200):\n",
    "        super().__init__(size=size, step_limit=step_limit)\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"achieved_goal\": gym.spaces.Box(\n",
    "                    0, self._size - 1, shape=(2,), dtype=int\n",
    "                ),\n",
    "                \"desired_goal\": gym.spaces.Discrete(self._num_colors),\n",
    "                \"square_colors\": gym.spaces.Box(\n",
    "                    0, self._num_colors - 1, shape=(self._size, self._size), dtype=int\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Override self._get_obs() to include achieved_goal and desired_goal. It's actually returning the same 3 things,\n",
    "    # we've just renamed agent location to 'achieved_goal' and target color to 'desired_goal' to match the interface.\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"achieved_goal\": self._agent_location,\n",
    "            \"desired_goal\": self._target_color,\n",
    "            \"square_colors\": self._square_colors,\n",
    "        }\n",
    "\n",
    "    # self.compute_reward function taking the achieved_goal and desired_goal as arguments\n",
    "    def compute_reward(self, achieved_goal, desired_goal, info):\n",
    "        square_colors = self._square_colors\n",
    "        target_color = desired_goal\n",
    "        agent_location = achieved_goal\n",
    "        print(\n",
    "            \"square colors\",\n",
    "            square_colors,\n",
    "            \"desired goal\",\n",
    "            target_color,\n",
    "            \"self._target_color\",\n",
    "            self._target_color,\n",
    "            \"achieved goal\",\n",
    "            agent_location,\n",
    "            \"self._agent_location\",\n",
    "            self._agent_location,\n",
    "            sep=\"\\n\",\n",
    "        )\n",
    "\n",
    "        reward = 1 if (square_colors[tuple(agent_location)] == target_color) else 0\n",
    "        return reward\n",
    "\n",
    "    # self.ompute_terminated will compute if the game is ended using achieved_goal, desired_goal, and external state.\n",
    "    def compute_terminated(self, achieved_goal, desired_goal, info):\n",
    "        # Since esentially terminated == bool(Reward), we can just call compute_reward and convert to bool.\n",
    "        terminated = bool(compute_reward(self, achieved_goal, desired_goal, info=None))\n",
    "        return terminated\n",
    "\n",
    "    # We must also include self.compute_truncated to fulfill the class, though that doesn't require achieved_goal/desired_goal.\n",
    "    def compute_truncated(self, achieved_goal, desired_goal, info):\n",
    "        return self.step_count > self.step_limit\n",
    "\n",
    "\n",
    "# Because implementing this interface improves the functionality of the game without changing any existing functionality,\n",
    "# I'm registering this env as SimpleColorGame-v1, and I'll be using it from now on.\n",
    "gym.register(\"SimpleColorGame-v1\", entry_point=SimpleColorGameGoalEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab37fd2a-eb07-473d-83ed-828365fb33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square colors\n",
      "[[1 0]\n",
      " [2 3]]\n",
      "desired goal\n",
      "[[1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]]\n",
      "self._target_color\n",
      "0\n",
      "achieved goal\n",
      "[[1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "self._agent_location\n",
      "[0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.compute_reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.compute_reward` for environment variables or `env.get_wrapper_attr('compute_reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 25 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleColorGame-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m mDQN \u001b[38;5;241m=\u001b[39m DQN(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m mDQN\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m     14\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m evaluate_policy(mPPO, env, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10,000 Training Timesteps mean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    268\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    269\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    270\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    272\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    273\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, gradient_steps\u001b[38;5;241m=\u001b[39mgradient_steps)\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:193\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    190\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gradient_steps):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# Sample replay buffer\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m     replay_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size, env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vec_normalize_env)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;66;03m# Compute the next Q-values using the target network\u001b[39;00m\n\u001b[0;32m    197\u001b[0m         next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net_target(replay_data\u001b[38;5;241m.\u001b[39mnext_observations)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\her\\her_replay_buffer.py:225\u001b[0m, in \u001b[0;36mHerReplayBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    223\u001b[0m real_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_real_samples(real_batch_indices, real_env_indices, env)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Create virtual transitions by sampling new desired goals and computing new rewards\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m virtual_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_virtual_samples(virtual_batch_indices, virtual_env_indices, env)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Concatenate real and virtual data\u001b[39;00m\n\u001b[0;32m    228\u001b[0m observations \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    229\u001b[0m     key: th\u001b[38;5;241m.\u001b[39mcat((real_data\u001b[38;5;241m.\u001b[39mobservations[key], virtual_data\u001b[38;5;241m.\u001b[39mobservations[key]))\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m virtual_data\u001b[38;5;241m.\u001b[39mobservations\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    231\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\her\\her_replay_buffer.py:320\u001b[0m, in \u001b[0;36mHerReplayBuffer._get_virtual_samples\u001b[1;34m(self, batch_indices, env_indices, env)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    318\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must initialize HerReplayBuffer with a VecEnv so it can compute rewards for virtual transitions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m# Compute new reward\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39menv_method(\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# the new state depends on the previous state and action\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# s_{t+1} = f(s_t, a_t)\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# so the next achieved_goal depends also on the previous state and action\u001b[39;00m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;66;03m# because we are in a GoalEnv:\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;66;03m# r_t = reward(s_t, a_t) = reward(next_achieved_goal, desired_goal)\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# therefore we have to use next_obs[\"achieved_goal\"] and not obs[\"achieved_goal\"]\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     next_obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124machieved_goal\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# here we use the new desired goal\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesired_goal\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    331\u001b[0m     infos,\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# we use the method of the first environment assuming that all environments are identical.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     indices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    334\u001b[0m )\n\u001b[0;32m    335\u001b[0m rewards \u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# env_method returns a list containing one element\u001b[39;00m\n\u001b[0;32m    336\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_obs(obs, env)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:129\u001b[0m, in \u001b[0;36mDummyVecEnv.env_method\u001b[1;34m(self, method_name, indices, *method_args, **method_kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call instance methods of vectorized environments.\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m target_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_envs(indices)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mgetattr\u001b[39m(env_i, method_name)(\u001b[38;5;241m*\u001b[39mmethod_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmethod_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m env_i \u001b[38;5;129;01min\u001b[39;00m target_envs]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call instance methods of vectorized environments.\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m target_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_envs(indices)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mgetattr\u001b[39m(env_i, method_name)(\u001b[38;5;241m*\u001b[39mmethod_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmethod_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m env_i \u001b[38;5;129;01min\u001b[39;00m target_envs]\n",
      "Cell \u001b[1;32mIn[45], line 47\u001b[0m, in \u001b[0;36mSimpleColorGameGoalEnv.compute_reward\u001b[1;34m(self, achieved_goal, desired_goal, info)\u001b[0m\n\u001b[0;32m     32\u001b[0m agent_location \u001b[38;5;241m=\u001b[39m achieved_goal\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare colors\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     square_colors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 47\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (square_colors[\u001b[38;5;28mtuple\u001b[39m(agent_location)] \u001b[38;5;241m==\u001b[39m target_color) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 25 were indexed"
     ]
    }
   ],
   "source": [
    "# For learning HER, I'll create a VecEnv of the 3x3 TCG and use a DQN for off-policy learning.\n",
    "\n",
    "env = gym.make(\"SimpleColorGame-v1\")\n",
    "mDQN = DQN(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(n_sampled_goal=4, goal_selection_strategy=\"future\"),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "mDQN.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(mPPO, env, n_eval_episodes=100)\n",
    "print(f\"10,000 Training Timesteps mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "mPPO.save(\"TCG0-s3-sl200-v10_DQN-HER-Te4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b766d9b-bec3-4b70-baea-498834dac6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
