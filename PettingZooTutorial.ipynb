{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73141ca6-5a10-459c-b177-e4846fcb044f",
   "metadata": {},
   "source": [
    "# PettingZoo Parallel API Custom Env Tutorial\n",
    "\n",
    "I'm going to complete a simple tutorial on creating/running custom Environments using Gymnasium's PettingZoo Parallel API.\n",
    "### Simple (Rock-Paper-Scissors)\n",
    "This simple game will cover the basics of parallel agent actions and observations, and calculating the reward structure.\n",
    "### Gridworld (Guard & Prisoner)\n",
    "This Gridworld game will be much closer to the Embodied Communication Game, and will teach me how to update the Env's internal logic after each joint call to the Env.step() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e439c1-f786-4c65-b612-26ae5a94f366",
   "metadata": {},
   "source": [
    "## Rock-Paper-Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feb8c536-8492-4777-9b5d-9e153fb59e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "from typing import Optional\n",
    "import functools\n",
    "import random as rng\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.utils import parallel_to_aec, wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db642aea-b2d6-45a7-a45f-184a82fe31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL CONSTANTS\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "MOVES = [\"ROCK\",\"PAPER\",\"SCISSORS\",\"None\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0,0),\n",
    "    (ROCK, PAPER): (-1,1),\n",
    "    (ROCK, SCISSORS): (1,-1),\n",
    "    (PAPER, PAPER): (0,0),\n",
    "    (PAPER, SCISSORS): (-1,1),\n",
    "    (PAPER, ROCK): (1,-1),\n",
    "    (SCISSORS, SCISSORS): (0,0),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "}\n",
    "\n",
    "#The Env function wraps the environment in some wrappers by default.\n",
    "def env(render_mode=None):\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    #This wrapper is meant only for Envs which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    #This wrapper helps error handling for discrete action spaces.\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    #This wrapper provides a variety of helpful user errors\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "#The raw_env function uses from_parallel to convert from ParallelEnv to AEC env.\n",
    "def raw_env(render_mode=None):\n",
    "    env = parallel_env(render_mode=render_mode)\n",
    "    env = parallel_to_aec(env)\n",
    "    return env\n",
    "\n",
    "class RockPaperScissors(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in envorinoment arguments and should defin the following attributes:\n",
    "        -self._possible_agents\n",
    "        -self._render_mode\n",
    "\n",
    "        Note: self.action_space and self.observation_space are now depracated. Action/Observation Spaces are\n",
    "        defined within the action_space() and observation_space() methods. These methods automatically return the\n",
    "        aforementioned variables, unless otherwise specified. So using them is fine unless we have a reason not to.\n",
    "        \"\"\"\n",
    "        #The player names\n",
    "        self._possible_agents = [\"player_\"+str(r) for r in range(2)]\n",
    "        \n",
    "        #render_mode, action_space, and observation_space will be public variables. All other variables will be private.\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "     #We will define the observation space as a function rather than a variable for this class.\n",
    "    #lru_cache allows the observation_space and action_space functions to be memoized for better performance\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return Discrete(4)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the Env. In human mode, it can print to terminal, open a graphical window, or open some other type\n",
    "        of display that's visible to the user.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling the render method, but haven't specified a render mode.\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        if len(self.agents) == 2:\n",
    "            string = f\"Current state: Agent1: {MOVES[self.state[self.agents[0]]]}, Agent2: {MOVES[self.state[self.agents[1]]]}\"\n",
    "        else:\n",
    "            string = \"Game Over.\"\n",
    "        print(string)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections, or any other environment data\n",
    "        which should not be kept around after the user is no longer using the Env. As we are not using any of these in\n",
    "        this version of the class, it currently does nothing.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets the Env to its initial state. Sets up the Env so that render() and step() can be called without issue.\n",
    "        Here it re-initializes the 'num_moves' variable which counts the number of hands played.\n",
    "        Returns the observations/infos for each agent.\n",
    "        \"\"\"\n",
    "        self._agents = self._possible_agents[:]\n",
    "        self._num_moves = 0\n",
    "        obs = {agent: NONE for agent in self._agents}\n",
    "        info = {agent: {} for agent in self._agents}\n",
    "        #We will use this variable to track the complete Env state, and update it within the step() method.\n",
    "        self._state = obs\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        step(action) takes an action for each agent as input and should return 5 variables:\n",
    "        [observations, rewards, terminated, truncated, infos]\n",
    "        each of these will be dicts containing one key per agent, like so:\n",
    "        {agent_1: item_1, agent_2: item_2}\n",
    "        \"\"\"\n",
    "        #If a user passes in actions containing no agents, then the returned dicts will be empty.\n",
    "        if not actions:\n",
    "            self._agents = []\n",
    "            return {}, {}, {}, {}, {}\n",
    "            \n",
    "        #Rewards for all agents are placed in a rewards dict to be returned.\n",
    "        rewards = {}\n",
    "        #I really dislike the fact that these methods are referencing external constants, but I'll build this\n",
    "        # the way the author wrote it for the purpose of completing this tutorial.\n",
    "        rewards[self._agents[0]], rewards[self._agents[1]] = REWARD_MAP[(\n",
    "            actions[self._agents[0]], actions[self._agents[1]]\n",
    "        )]\n",
    "\n",
    "        terminations = {agent: env_truncation for agent in self._agents}\n",
    "\n",
    "        observations = {\n",
    "            self._agents[i]: int(actions[self._agents[1-i]]) for i in range(len(self.agents))\n",
    "        }\n",
    "\n",
    "        self._state = observations\n",
    "\n",
    "        #typically there won't be any information in infos, but step() must still return an info for each agent.\n",
    "        infos = {agent: {} for agent in self._agents}\n",
    "\n",
    "        if env_truncation:\n",
    "            self._agents = []\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769562f-640c-4f88-b5e0-f2136c8019c0",
   "metadata": {},
   "source": [
    "### Well, now we've made a parallel environment. \n",
    "Now the question is how to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8005f6d4-c1d9-4790-b087-2c835e59415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'player_0': 3, 'player_1': 3}, {'player_0': {}, 'player_1': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RockPaperScissors()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc79175-a7a7-4ab6-bcb2-902f8c187690",
   "metadata": {},
   "source": [
    "This is an odd procedure for creating an running the environment. I'm a much bigger fan of the gym.register() and gym.make() approach. Unfortunately, this approach produces an error upon trying to run gym.make(), presumably because a **ParallelEnv** doesn't subscribe to the same interfaces as a normal Gymnasium Env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a6e9b60-b6b4-47e5-82e1-c0e2a364c766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment RockPaperScissors-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "action space does not inherit from `gymnasium.spaces.Space`, actual type: <class 'method'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m gym\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRockPaperScissors-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      2\u001b[0m              entry_point \u001b[38;5;241m=\u001b[39m RockPaperScissors)\n\u001b[1;32m----> 3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRockPaperScissors-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\envs\\registration.py:861\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# Run the environment checker as the lowest level wrapper\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    859\u001b[0m     disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m env_spec\u001b[38;5;241m.\u001b[39mdisable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m ):\n\u001b[1;32m--> 861\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mPassiveEnvChecker(env)\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# Add the order enforcing wrapper\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_spec\u001b[38;5;241m.\u001b[39morder_enforce:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:34\u001b[0m, in \u001b[0;36mPassiveEnvChecker.__init__\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m     29\u001b[0m gym\u001b[38;5;241m.\u001b[39mWrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m     32\u001b[0m     env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment must specify an action space. https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 34\u001b[0m check_action_space(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m     36\u001b[0m     env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation_space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment must specify an observation space. https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m check_observation_space(env\u001b[38;5;241m.\u001b[39mobservation_space)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:68\u001b[0m, in \u001b[0;36mcheck_space\u001b[1;34m(space, space_type, check_box_space_fn)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check of the environment action space that should not affect the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space, spaces\u001b[38;5;241m.\u001b[39mSpace):\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspace_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m space does not inherit from `gymnasium.spaces.Space`, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(space)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m     73\u001b[0m     check_box_space_fn(space)\n",
      "\u001b[1;31mAssertionError\u001b[0m: action space does not inherit from `gymnasium.spaces.Space`, actual type: <class 'method'>"
     ]
    }
   ],
   "source": [
    "# gym.register(\"RockPaperScissors-v0\",\n",
    "#              entry_point = RockPaperScissors)\n",
    "# env = gym.make(\"RockPaperScissors-v0\")\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520ac63-1ab7-4025-b439-d60650ac4d25",
   "metadata": {},
   "source": [
    "Let's try making the next Env, **Guard and Prisoner**. We'll put more effort into running that Env successfully with the SB3 RL models, since, being a Grid-World, it's much more similar to the Embodied Communication Game.\n",
    "\n",
    "## Gridworld (Guard and Prisoner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2deebeb2-be9b-48bb-a753-1f9d09b1f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuardAndPrisoner(ParallelEnv):\n",
    "    \"\"\"\n",
    "    metadata holds Env constants. \"name\" metadata allows Env to be pretty printed.\n",
    "    \"\"\"\n",
    "    metadata = { \"name\": \"custom_environment_v0\" }\n",
    "    def __init__(self, height = 5, width = 5, step_limit = 100):\n",
    "        \"\"\"\n",
    "        Takes in Env arguments.\n",
    "\n",
    "        Should define the following:\n",
    "\n",
    "        -Escape Coords\n",
    "        -Prisoner starting Coords\n",
    "        -Guard starting Coords\n",
    "        -Timestamp\n",
    "        -Possible_Agents\n",
    "        \"\"\"\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "        self._escape_coords = np.array([-1, -1], dtype=np.int32)\n",
    "        self._guard_coords = np.array([-1,-1],dtype=np.int32)\n",
    "        self._prisoner_coords = np.array([-1,-1],dtype=np.int32)\n",
    "        self._timestep = 0\n",
    "        self._step_limit = step_limit\n",
    "        self._possible_agents = [\"prisoner\",\"guard\"]\n",
    "\n",
    "        #Dictionary to map action space to a direction on the Gridworld.\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([0,-1]), #Down\n",
    "            1: np.array([1,0]), #Right\n",
    "            2: np.array([0,1]), #Up\n",
    "            3: np.arrau([-1,0]), #Left\n",
    "        }\n",
    "\n",
    "    #Farama recommends defining the Spaces directly in a getter function, rather than relying on self.x_space variables.\n",
    "    #Defining observation_space in a memo-ized getter function for best performance.\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self,agent):\n",
    "        return gym.spaces.Tuple((\n",
    "            Box(0, max([self._height, self._width]) - 1, shape=(2,), dtype=int),\n",
    "            Box(0, max([self._height, self._width]) - 1, shape=(2,), dtype=int),\n",
    "            Box(0, max([self._height, self._width]) - 1, shape=(2,), dtype=int),\n",
    "        ))\n",
    "   #Defining action_space in a memo-ized getter function.\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self,agent):\n",
    "        return Discrete(4)\n",
    "\n",
    "    #Helper function to return agent observations. We will call this in self.reset() and self.step()\n",
    "    def _get_obs(self):\n",
    "        observations = {\n",
    "            a : (\n",
    "                self._escape_coords,\n",
    "                self._prisoner_coords,\n",
    "                self.guard_coords\n",
    "            ) for a in self.agents\n",
    "        }\n",
    "        return observations\n",
    "\n",
    "    #Helper function to return infos. This doesn't currently return anything, but can be useful for auxiliary info.\n",
    "    def _get_infos(self):\n",
    "        infos = {a : {} for a in self.agents}\n",
    "        return infos\n",
    "\n",
    "    #Helper function to calculate rewards and terminateds, because terminated is true whenever rewards are doled out. Called in self.step()\n",
    "    def _get_rewards_and_terminateds(self):\n",
    "        terminateds = {a : False for a in self.agents}\n",
    "        rewards = {a : 0 for a in self.agents}\n",
    "        #Reward if the prisoner escapes. In this version of the game, a tie goes to the runner.\n",
    "        if(self._prisoner_coords == self._escape_coords):\n",
    "            rewards = {\"prisoner\": 1, \"guard\": -1}\n",
    "            terminateds = {a : True for a in self.agents}\n",
    "        #Reward if the guard catches the prisoner.\n",
    "        elif(self._guard_coords == self._prisoner_coords):\n",
    "            rewards = {\"prisoner\": -1, \"guard\": 1}\n",
    "            terminateds = {a : True for a in self.agents}\n",
    "        return rewards, terminateds\n",
    "\n",
    "    #Helper function to get Truncateds\n",
    "    def _get_truncateds(self):\n",
    "        if(self._timestep > self._step_limit):\n",
    "            truncateds = {\"prisoner\": True, \"guard\": True}\n",
    "        else:\n",
    "            truncateds = {\"prisoner\": False, \"guard\": False}\n",
    "        return truncateds\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        Reset Env to a randomized starting configuration.\n",
    "        Must initialize the following:\n",
    "\n",
    "        -agents\n",
    "        -timestamp\n",
    "        -prisoner coords\n",
    "        -guard coords\n",
    "        -escape coords\n",
    "        -observations\n",
    "        -infos\n",
    "        \"\"\"\n",
    "        #Call super.reset() to seed the RNG with seed parameter.\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        #Start self.agents with self._possible_agents and self._timestep at 0.\n",
    "        self.agents = self._possible_agents\n",
    "        self._timestep = 0\n",
    "        \n",
    "        #Use the set() object to generate three unique coords to assign to guard, prisoner, and escape.\n",
    "        coords = set()\n",
    "        while len(coords) < 3:\n",
    "            x = self.np_random.integers(0, self._width)\n",
    "            y = self.np_random.integers(0, self._height)\n",
    "            coords.add((x,y))\n",
    "            \n",
    "        coords = np.array(list(coords))\n",
    "        \n",
    "        #instantiate the guard, prisoner, and escape coords.\n",
    "        self._escape_coords = coords[0]\n",
    "        self._guard_coords = coords[1]\n",
    "        self._prisoner_coords = coords[2]\n",
    "        \n",
    "        #Get the agent observations and auxiliary info and return it.\n",
    "        obs = self._get_obs()\n",
    "        infos = self._get_infos()\n",
    "        \n",
    "        return obs,infos\n",
    "\n",
    "    def step(self,action):\n",
    "        \"\"\"\n",
    "        This function takes in a dictionary of actions as an argument and updates the Env according to the actions.\n",
    "        The actions dict contains two actions and looks something like this:\n",
    "        {\n",
    "            \"guard\": Guard Action,\n",
    "            \"prisoner\": Prisoner Action,\n",
    "        }\n",
    "        Must update:\n",
    "        -prisoner and guard coords\n",
    "        -terminations and rewards if guard reaches prisoner or prisoner reaches escape.\n",
    "        \n",
    "        This function returns 5 variables:\n",
    "        Observations, Rewards, Terminateds, Truncateds, Infos\n",
    "        \"\"\"\n",
    "        prisoner_direction = self._action_to_direction[actions[\"prisoner\"]]\n",
    "        guard_direction = self._action_to_directoin[actions[\"guard\"]]\n",
    "        \n",
    "        #Update state based on agent actions.\n",
    "        \n",
    "        #To update the agent locations, we must add the direction coords to their coords, and apply np.clip to ensure\n",
    "        #The agents are still in-bounds.\n",
    "        self._prisoner_coords = np.clip(self._prisoner_coords+prisoner_direction,[0,0],[self._width-1,self._height-1])\n",
    "        self._guard_coords = np.clip(self._guard_coords+guard_direction,[0,0],[self._width-1,self._height-1])\n",
    "        \n",
    "        #Check the new state for rewards, terminateds, and truncateds, and generate the return values.\n",
    "        observations = self._get_obs()\n",
    "        infos = self._get_infos()\n",
    "        rewards, terminateds = self._get_rewards_and_terminateds()\n",
    "        truncateds = self._get_truncateds()\n",
    "\n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf3ecc-6a7c-4b98-8a03-77c259881062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
