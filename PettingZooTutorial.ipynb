{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73141ca6-5a10-459c-b177-e4846fcb044f",
   "metadata": {},
   "source": [
    "# PettingZoo Parallel API Custom Env Tutorial\n",
    "\n",
    "I'm going to complete a simple tutorial on creating/running custom Environments using Gymnasium's PettingZoo Parallel API.\n",
    "### Simple (Rock-Paper-Scissors)\n",
    "This simple game will cover the basics of parallel agent actions and observations, and calculating the reward structure.\n",
    "### Gridworld (Guard & Prisoner)\n",
    "This Gridworld game will be much closer to the Embodied Communication Game, and will teach me how to update the Env's internal logic after each joint call to the Env.step() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e439c1-f786-4c65-b612-26ae5a94f366",
   "metadata": {},
   "source": [
    "## Rock-Paper-Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb8c536-8492-4777-9b5d-9e153fb59e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#MISC\n",
    "from copy import copy\n",
    "from typing import Optional\n",
    "import functools\n",
    "import random as rng\n",
    "import numpy as np\n",
    "\n",
    "#GYMNASIUM / PETTINGZOO\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "from gymnasium.utils import seeding\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from pettingzoo.test import parallel_api_test\n",
    "from pettingzoo.utils import parallel_to_aec, wrappers\n",
    "\n",
    "#SB3\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db642aea-b2d6-45a7-a45f-184a82fe31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL CONSTANTS\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "MOVES = [\"ROCK\",\"PAPER\",\"SCISSORS\",\"None\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0,0),\n",
    "    (ROCK, PAPER): (-1,1),\n",
    "    (ROCK, SCISSORS): (1,-1),\n",
    "    (PAPER, PAPER): (0,0),\n",
    "    (PAPER, SCISSORS): (-1,1),\n",
    "    (PAPER, ROCK): (1,-1),\n",
    "    (SCISSORS, SCISSORS): (0,0),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "}\n",
    "\n",
    "#The Env function wraps the environment in some wrappers by default.\n",
    "def env(render_mode=None):\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    #This wrapper is meant only for Envs which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    #This wrapper helps error handling for discrete action spaces.\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    #This wrapper provides a variety of helpful user errors\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "#The raw_env function uses from_parallel to convert from ParallelEnv to AEC env.\n",
    "def raw_env(render_mode=None):\n",
    "    env = parallel_env(render_mode=render_mode)\n",
    "    env = parallel_to_aec(env)\n",
    "    return env\n",
    "\n",
    "class RockPaperScissors(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in envorinoment arguments and should defin the following attributes:\n",
    "        -self.possible_agents\n",
    "        -self._render_mode\n",
    "\n",
    "        Note: self.action_space and self.observation_space are now depracated. Action/Observation Spaces are\n",
    "        defined within the action_space() and observation_space() methods. These methods automatically return the\n",
    "        aforementioned variables, unless otherwise specified. So using them is fine unless we have a reason not to.\n",
    "        \"\"\"\n",
    "        #The player names\n",
    "        self.possible_agents = [\"player_\"+str(r) for r in range(2)]\n",
    "        \n",
    "        #render_mode, action_space, and observation_space will be public variables. All other variables will be private.\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "     #We will define the observation space as a function rather than a variable for this class.\n",
    "    #lru_cache allows the observation_space and action_space functions to be memoized for better performance\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return Discrete(4)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the Env. In human mode, it can print to terminal, open a graphical window, or open some other type\n",
    "        of display that's visible to the user.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling the render method, but haven't specified a render mode.\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        if len(self.agents) == 2:\n",
    "            string = f\"Current state: Agent1: {MOVES[self.state[self.agents[0]]]}, Agent2: {MOVES[self.state[self.agents[1]]]}\"\n",
    "        else:\n",
    "            string = \"Game Over.\"\n",
    "        print(string)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections, or any other environment data\n",
    "        which should not be kept around after the user is no longer using the Env. As we are not using any of these in\n",
    "        this version of the class, it currently does nothing.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets the Env to its initial state. Sets up the Env so that render() and step() can be called without issue.\n",
    "        Here it re-initializes the 'num_moves' variable which counts the number of hands played.\n",
    "        Returns the observations/infos for each agent.\n",
    "        \"\"\"\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self._num_moves = 0\n",
    "        obs = {agent: None for agent in self.agents}\n",
    "        info = {agent: {} for agent in self.agents}\n",
    "        #We will use this variable to track the complete Env state, and update it within the step() method.\n",
    "        self._state = obs\n",
    "\n",
    "        return (obs, info)\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        step(action) takes an action for each agent as input and should return 5 variables:\n",
    "        [observations, rewards, terminated, truncated, infos]\n",
    "        each of these will be dicts containing one key per agent, like so:\n",
    "        {agent_1: item_1, agent_2: item_2}\n",
    "        \"\"\"\n",
    "        #If a user passes in actions containing no agents, then the returned dicts will be empty.\n",
    "        if not actions:\n",
    "            self.agents = []\n",
    "            return {}, {}, {}, {}, {}\n",
    "            \n",
    "        #Rewards for all agents are placed in a rewards dict to be returned.\n",
    "        rewards = {}\n",
    "        #I really dislike the fact that these methods are referencing external constants, but I'll build this\n",
    "        # the way the author wrote it for the purpose of completing this tutorial.\n",
    "        rewards[self.agents [0]], rewards[self.agents [1]] = REWARD_MAP[(\n",
    "            actions[self.agents [0]], actions[self.agents [1]]\n",
    "        )]\n",
    "\n",
    "        terminations = {agent: env_truncation for agent in self.agents}\n",
    "\n",
    "        observations = {\n",
    "            self.agents [i]: int(actions[self.agents [1-i]]) for i in range(len(self.agents))\n",
    "        }\n",
    "\n",
    "        self._state = observations\n",
    "\n",
    "        #typically there won't be any information in infos, but step() must still return an info for each agent.\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        if env_truncation:\n",
    "            self.agents = []\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769562f-640c-4f88-b5e0-f2136c8019c0",
   "metadata": {},
   "source": [
    "### Well, now we've made a parallel environment. \n",
    "Now the question is how to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8005f6d4-c1d9-4790-b087-2c835e59415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'player_0': 3, 'player_1': 3}, {'player_0': {}, 'player_1': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RockPaperScissors()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc79175-a7a7-4ab6-bcb2-902f8c187690",
   "metadata": {},
   "source": [
    "This is an odd procedure for creating an running the environment. I'm a much bigger fan of the gym.register() and gym.make() approach. Unfortunately, this approach produces an error upon trying to run gym.make(), presumably because a **ParallelEnv** doesn't subscribe to the same interfaces as a normal Gymnasium Env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520ac63-1ab7-4025-b439-d60650ac4d25",
   "metadata": {},
   "source": [
    "Let's try making the next Env, **Guard and Prisoner**. We'll put more effort into running that Env successfully with the SB3 RL models, since, being a Grid-World, it's much more similar to the Embodied Communication Game.\n",
    "\n",
    "## Gridworld (Guard and Prisoner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2deebeb2-be9b-48bb-a753-1f9d09b1f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuardAndPrisoner(ParallelEnv):\n",
    "    \"\"\"\n",
    "    metadata holds Env constants. \"name\" metadata allows Env to be pretty printed.\n",
    "    \"\"\"\n",
    "    metadata = { \"render_modes\": [],\n",
    "                \"name\": \"guard_and_prisoner_v0\" }\n",
    "    def __init__(self, height = 5, width = 5, step_limit = 100):\n",
    "        \"\"\"\n",
    "        Takes in Env arguments.\n",
    "\n",
    "        Should define the following:\n",
    "\n",
    "        -Escape Coords\n",
    "        -Prisoner starting Coords\n",
    "        -Guard starting Coords\n",
    "        -Timestamp\n",
    "        -Possible_Agents\n",
    "        \"\"\"\n",
    "        #Private Variables\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "        self._escape_coords = np.array([-1, -1], dtype=np.int32)\n",
    "        self._guard_coords = np.array([-1,-1],dtype=np.int32)\n",
    "        self._prisoner_coords = np.array([-1,-1],dtype=np.int32)\n",
    "        self._timestep = 0\n",
    "        self._step_limit = step_limit\n",
    "        \n",
    "        #Public Variables (necessary for ParallelEnv implementation)\n",
    "        self.possible_agents = [\"prisoner\",\"guard\"]\n",
    "        self.render_mode = None\n",
    "\n",
    "        #Dictionary to map action space to a direction on the Gridworld.\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([0,-1]), #Down\n",
    "            1: np.array([1,0]), #Right\n",
    "            2: np.array([0,1]), #Up\n",
    "            3: np.array([-1,0]), #Left\n",
    "        }\n",
    "\n",
    "    #Because this Environment doesn't actually extend the base Env class, I can't set the seed\n",
    "    # by calling super().reset(). Instead, this function will manually set the seed.\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "\n",
    "    #Farama recommends defining the Spaces directly in a getter function, rather than relying on self.x_space variables.\n",
    "    #Defining observation_space in a memo-ized getter function for best performance.\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self,agent):\n",
    "        return gym.spaces.Dict({\n",
    "            \"escape\" : gym.spaces.Box(0, max([self._height, self._width]) - 1, shape=(2,), dtype=int),\n",
    "            \"prisoner\" : gym.spaces.Box(0, max([self._height, self._width]) - 1, shape=(2,), dtype=int),\n",
    "            \"guard\" : gym.spaces.Box(0, max([self._height, self._width]) - 1, shape=(2,), dtype=int),\n",
    "        })\n",
    "   #Defining action_space in a memo-ized getter function.\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self,agent):\n",
    "        return Discrete(4)\n",
    "\n",
    "    #Helper function to return agent observations. We will call this in self.reset() and self.step()\n",
    "    def _get_obs(self):\n",
    "        observations = {\n",
    "            a : {\n",
    "                \"escape\": self._escape_coords,\n",
    "                \"prisoner\": self._prisoner_coords,\n",
    "                \"guard\": self._guard_coords,\n",
    "            } for a in self.agents\n",
    "        }\n",
    "        return observations\n",
    "\n",
    "    #Helper function to return infos. This doesn't currently return anything, but can be useful for auxiliary info.\n",
    "    def _get_infos(self):\n",
    "        infos = {a : {} for a in self.agents}\n",
    "        return infos\n",
    "\n",
    "    #Helper function to calculate rewards and terminateds, because terminated is true whenever rewards are doled out. Called in self.step()\n",
    "    def _get_rewards_and_terminateds(self):\n",
    "        terminateds = {a : False for a in self.agents}\n",
    "        rewards = {a : 0 for a in self.agents}\n",
    "        #Reward if the prisoner escapes. In this version of the game, a tie goes to the runner.\n",
    "        if(np.array_equal(self._prisoner_coords, self._escape_coords)):\n",
    "            rewards = {\"prisoner\": 1, \"guard\": 0}\n",
    "            terminateds = {a : True for a in self.agents}\n",
    "        #Reward if the guard catches the prisoner.\n",
    "        elif(np.array_equal(self._guard_coords, self._prisoner_coords)):\n",
    "            rewards = {\"prisoner\": 0, \"guard\": 1}\n",
    "            terminateds = {a : True for a in self.agents}\n",
    "        return rewards, terminateds\n",
    "\n",
    "    #Helper function to get Truncateds\n",
    "    def _get_truncateds(self):\n",
    "        if(self._timestep > self._step_limit):\n",
    "            truncateds = {\"prisoner\": True, \"guard\": True}\n",
    "        else:\n",
    "            truncateds = {\"prisoner\": False, \"guard\": False}\n",
    "        return truncateds\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        Reset Env to a randomized starting configuration.\n",
    "        Must initialize the following:\n",
    "\n",
    "        -agents\n",
    "        -timestamp\n",
    "        -prisoner coords\n",
    "        -guard coords\n",
    "        -escape coords\n",
    "        -observations\n",
    "        -infos\n",
    "        \"\"\"\n",
    "        #If the seed is not None, call _seed(seed) to set self.np_random\n",
    "        self._seed(seed)\n",
    "        \n",
    "        #Start self.agents with a copy of self.possible_agents and set self._timestep to 0.\n",
    "        self.agents = copy(self.possible_agents)\n",
    "        self._timestep = 0\n",
    "        \n",
    "        #Use the set() object to generate three unique coords to assign to guard, prisoner, and escape.\n",
    "        coords = set()\n",
    "        while len(coords) < 3:\n",
    "            x = self.np_random.integers(0, self._width)\n",
    "            y = self.np_random.integers(0, self._height)\n",
    "            coords.add((x,y))\n",
    "            \n",
    "        coords = np.array(list(coords))\n",
    "        \n",
    "        #instantiate the guard, prisoner, and escape coords.\n",
    "        self._escape_coords = coords[0]\n",
    "        self._guard_coords = coords[1]\n",
    "        self._prisoner_coords = coords[2]\n",
    "        \n",
    "        #Get the agent observations and auxiliary info and return it.\n",
    "        obs = self._get_obs()\n",
    "        infos = self._get_infos()\n",
    "        \n",
    "        return (obs,infos)\n",
    "\n",
    "    def step(self,actions):\n",
    "        \"\"\"\n",
    "        This function takes in a dictionary of actions as an argument and updates the Env according to the actions.\n",
    "        The actions dict contains two actions and looks something like this:\n",
    "        {\n",
    "            \"guard\": Guard Action,\n",
    "            \"prisoner\": Prisoner Action,\n",
    "        }\n",
    "        Must update:\n",
    "        -prisoner and guard coords\n",
    "        -terminations and rewards if guard reaches prisoner or prisoner reaches escape.\n",
    "        \n",
    "        This function returns 5 variables:\n",
    "        Observations, Rewards, Terminateds, Truncateds, Infos\n",
    "        \"\"\"\n",
    "        prisoner_direction = self._action_to_direction[actions[\"prisoner\"]]\n",
    "        guard_direction = self._action_to_direction[actions[\"guard\"]]\n",
    "        \n",
    "        #Update state based on agent actions.\n",
    "\n",
    "        #Firstly, increment self._step_count\n",
    "        self._timestep += 1\n",
    "        \n",
    "        #To update the agent locations, we must add the direction coords to their coords, and apply np.clip to ensure\n",
    "        #The agents are still in-bounds.\n",
    "        self._prisoner_coords = np.clip(self._prisoner_coords+prisoner_direction,[0,0],[self._width-1,self._height-1])\n",
    "        self._guard_coords = np.clip(self._guard_coords+guard_direction,[0,0],[self._width-1,self._height-1])\n",
    "        \n",
    "        #Check the new state for rewards, terminateds, and truncateds, and generate the return values.\n",
    "        observations = self._get_obs()\n",
    "        infos = self._get_infos()\n",
    "        rewards, terminateds = self._get_rewards_and_terminateds()\n",
    "        truncateds = self._get_truncateds()\n",
    "        \n",
    "        #Apparently you have to empty out self.agents[] if the game is terminated or truncated.\n",
    "        # I'm not sure why to be honest, but that's an API standard.\n",
    "        if all(terminateds.values()) or all(truncateds.values()):\n",
    "            self.agents = []\n",
    "\n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42cf3ecc-6a7c-4b98-8a03-77c259881062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Parallel API test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4adbd1a274487bb8971d0a6fb07dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline performance. Mean reward of 0.1 +/- 0.3\n",
      "starting training on guard_and_prisoner_v0.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance after 100000 training timesteps. Mean reward of 0.43 +/- 0.4950757517794625\n"
     ]
    }
   ],
   "source": [
    "#Trying to create an Environment which multiple PPO models can train in using SuperSuit's converter\n",
    "# functions for PettingZoo Envs.\n",
    "env = GuardAndPrisoner()\n",
    "\n",
    "#First, we must pass the Parallel_API_Test to make sure we are implementing ParallelEnv correctly. We\n",
    "# made quite a few errors in our initial implementation of the Environment, including calling\n",
    "# super().reset() to an Env which did not extend the base Env class (ParallelEnv doesn't extend Env for some reason)\n",
    "# Not realizing that self.agents and self.possible_agents are essential variables used by the\n",
    "# ParallelEnv API and not just some random variable names used by the guy who wrote the tutorial.\n",
    "# They must be named self.agents and self.possible agents, and they must be created and destroyed\n",
    "# at very specific points (E.G. The trickiest mistake for me to fix was learning that I had to reset\n",
    "# self.agents to [] within self.step() whenever the game was terminated or truncated. It really wasn't\n",
    "# obvious that that was causing the error in the test, and I had to look back through the tutorial code\n",
    "# to figure out what I was doing wrong.\n",
    "parallel_api_test(env, num_cycles=1000)\n",
    "\n",
    "#Now that our ParallelEnv passes the API test, we want to use SuperSuit to hook it up to SB3's PPO models.\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, 1, base_class=\"stable_baselines3\")\n",
    "\n",
    "mPPO = PPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    verbose = 0\n",
    ")\n",
    "mean, std = evaluate_policy(mPPO, env, n_eval_episodes=100)\n",
    "print(f\"baseline performance. Mean reward of {mean} +/- {std}\")\n",
    "print(f\"starting training on {str(env.venv.metadata['name'])}.\")\n",
    "\n",
    "mPPO.learn(total_timesteps=100000, progress_bar = True)\n",
    "mean, std = evaluate_policy(mPPO, env, n_eval_episodes=100)\n",
    "print(f\"performance after 100000 training timesteps. Mean reward of {mean} +/- {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead34cfb-8db9-4b36-aea4-1cea49542a42",
   "metadata": {},
   "source": [
    "It looks like it did learn in the parallelized VecEnv! However, the strength of this approach is severely limited because both 'agents' are actually the same model, and so the must learn together and have identical strategies. This means that this learning process will only achieve the optimal strategy in environments which can be solved with two identical strategies, i.e. \"symmetric\" games. This game is fundamentally asymmetric, as the prisoner wants to escape and the guard wants to catch the prisoner. My guess is that the model made a beeline for the exit with both the guard and the prisoner, and as a result was able to receive some kind of reward each run. Fortunately, the **ECG** can be solved either symmetrically or asymmetrically, so I have hope that this training method will work for it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
