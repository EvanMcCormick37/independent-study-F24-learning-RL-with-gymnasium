{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77604744-d0d8-48e7-996e-e8515203ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Imports\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, PPO, DQN\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import pygame\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32b130-134d-4f9f-86e5-d0c73b03732c",
   "metadata": {},
   "source": [
    "# Goals for this Stage of Research\n",
    "* Create and render three different Gymnasium environments\n",
    "* Train three different StableBaseline3 RL models on each environment\n",
    "* Measure performance of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134d340-b02e-42f8-bb27-a9c5603a4ebd",
   "metadata": {},
   "source": [
    "### 1. Taxi-V3\n",
    "Here we're running the Taxi-v3 environment in Gymnasium with StableBaseline3's VecEnv (Vectorized Environment) which allows for simultaneous training on multiple instances of a Gymnasium environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a260a72-7785-41d6-b988-e9361af7dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -1097.48 +/- 897.50\n"
     ]
    }
   ],
   "source": [
    "#Add in render_mode = \"human\" to have PyGame display the environment as it progresses\n",
    "vec_env = make_vec_env(\"Taxi-v3\",n_envs=4)\n",
    "model1 = PPO(MlpPolicy, vec_env, verbose=0)\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ab7fa-77ae-44a5-be7b-8e676215da4a",
   "metadata": {},
   "source": [
    "Here we're training a StableBaselines3 PPO Model to solve the Taxi environment. We can see significant improvement in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8375f637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "model1.learn(total_timesteps=10000)\n",
    "mean_reward, std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model1.predict(obs)\n",
    "    obs, reward, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "vec_env.close()\n",
    "model.save(\"PPO_Taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8faaed8-dcc7-4d80-b22e-d236f6aa3d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -1205.30 +/- 891.11\n"
     ]
    }
   ],
   "source": [
    "model2 = A2C(\"MlpPolicy\", vec_env, verbose=0)\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d240e367-1788-4f1a-83b9-5ebd41482f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "model2.learn(total_timesteps=10000)\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model2.save(\"A2C_Taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1acd3749-601c-40d6-acc6-f28a3630d337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -1963.91 +/- 251.99\n"
     ]
    }
   ],
   "source": [
    "model3 = DQN(\"MlpPolicy\", vec_env, verbose=0)\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ade119c-12de-4a4b-af69-608b4a7b6e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "model3.learn(total_timesteps=10000)\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.save(\"DQN_Taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42b606-3d80-498f-82a2-029c96420aad",
   "metadata": {},
   "source": [
    "We trained all three types of models and all got a mean reward of -200, which appears to be the maximum. All three seem to be viable algorithms for this game. Let's move on to another environment.\n",
    "\n",
    "### Cartpole-v1\n",
    "\n",
    "Let's now test three models (A2C, PPO, and DQN) in Cartpole-v1 to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc6a99f7-a3ed-463a-9d73-126f8d130729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 9.33 +/- 0.79\n",
      "mean reward: 9.38 +/- 0.72\n"
     ]
    }
   ],
   "source": [
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "model1 = DQN(\"MlpPolicy\", vec_env, verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.save(\"DQN_CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c083ac61-26ec-47cd-ac63-d7a795bd651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 81.94 +/- 35.55\n",
      "mean reward: 249.13 +/- 58.29\n"
     ]
    }
   ],
   "source": [
    "model2 = A2C(\"MlpPolicy\", vec_env, verbose=0)\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model2.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model2.save(\"A2C_CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a06fec1-642c-439e-9d46-b2f8edb82006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 8.87 +/- 0.64\n",
      "mean reward: 9.43 +/- 0.70\n"
     ]
    }
   ],
   "source": [
    "model3 = PPO(\"MlpPolicy\", vec_env, verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.save(\"PPO_CartPole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c54502-8576-42bf-bef9-05fad8c63d57",
   "metadata": {},
   "source": [
    "Interestingly, the A2C model out-performed PPO and DQN in the CartPole environment. Finally, we have a third environment..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336e4dd-2e6f-4bcb-aeff-74f9a66eeb64",
   "metadata": {},
   "source": [
    "### FrozenLake-v1\n",
    "This is another simple environment, which includes an element of randomness in how the model interacts with its' environment. In this environment, the agent attempts to reach a finish goal by moving across a slippery ice-field, trying to avoid holes in the ice. Due to the 'slipperiness' of the ice, the agent has a small chance of randomly moving in a direction other than the one chosen by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "197da738-b8bf-4daa-9ebe-4efa6bee5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = make_vec_env('FrozenLake-v1', n_envs=4)\n",
    "vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b474642-90c7-4f4a-a9f6-6d8a0134c637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 0.00 +/- 0.00\n",
      "mean reward: 0.04 +/- 0.20\n"
     ]
    }
   ],
   "source": [
    "model1 = PPO('MlpPolicy',vec_env,verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.save(\"PPO_FrozenLake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "887a7499-8af2-4196-8f01-bcc3b34b4574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 0.00 +/- 0.00\n",
      "mean reward: 0.14 +/- 0.35\n"
     ]
    }
   ],
   "source": [
    "model2 = A2C('MlpPolicy',vec_env,verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model2.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model2.save(\"A2C_FrozenLake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a870b93-b057-41eb-aa0f-65cb8b23b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 0.05 +/- 0.22\n",
      "mean reward: 0.07 +/- 0.26\n"
     ]
    }
   ],
   "source": [
    "model3 = DQN('MlpPolicy',vec_env,verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.save(\"DQN_FrozenLake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3463cb7d-11db-4319-a8c2-1b4d1fe783c8",
   "metadata": {},
   "source": [
    "Again, it seems the A2C model out-performed the other two models. This could be due to the A2C model having better training performance on a Vectorized Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be6d48-fa97-49ac-80da-c21da9827d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9624cbc-cebb-466a-bb36-6fc7467c1d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 0.00 +/- 0.00\n",
      "mean reward: 0.18 +/- 0.38\n"
     ]
    }
   ],
   "source": [
    "model1 = PPO('MlpPolicy',env,verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.save(\"PPO_FrozenLake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a95a0f0e-d1e9-40ad-a657-ef1dcb2ec423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 0.02 +/- 0.14\n",
      "mean reward: 0.27 +/- 0.44\n"
     ]
    }
   ],
   "source": [
    "model3 = DQN('MlpPolicy',env,verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.save(\"DQN_FrozenLake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865543d2-6047-4408-bae6-57baaf4bb501",
   "metadata": {},
   "source": [
    "Yup. it appears that the DQN and PPO models are not designed to be trained on Vectorized environments, at least not for some gymnasium envs. So, vectorizing environments is only helpful in some circumstances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
