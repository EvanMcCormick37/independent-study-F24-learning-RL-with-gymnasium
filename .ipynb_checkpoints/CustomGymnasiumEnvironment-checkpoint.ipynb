{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ac7bbb-2a99-4502-a494-c54e51dc1c09",
   "metadata": {},
   "source": [
    "# Custom Environments and Wrappers in Gymnasium\n",
    "\n",
    "Before I can build and run a version of the Embodied Communication Game, I must learn how to build custom environments in Gymnasium. Here I'm going to run through a few tutorials on building custom `Envs` and modifying them with custom `wrappers`.\n",
    "\n",
    "## Designing Custom Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a963b5-9b41-4420-9008-32879872169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f34a9a-dc3b-4922-8971-13f38a033c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment GridWorld-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "\n",
    "    #Initializes the environment with specific attributes including size, observation_space, action_space, and any other variables \n",
    "    # defining the agent, environment, or reward structure.\n",
    "    def __init__(self, size: int = 5):\n",
    "        # The size of the square grid\n",
    "        self.size = size\n",
    "\n",
    "        # Define the agent and target location; randomly chosen in `reset` and updated in `step`\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "    #A common design pattern is to include a _get_obs method for translating state into an observation. However, this helper method\n",
    "    # is not mandatory, and you might want to compute observations directly in env.reset and env.step, which may be preferable if \n",
    "    # you want to compute them differently in each method call.\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    \n",
    "    #A similar pattern, _get_info can be used to return auxiliary information. In this Env, we would like to calculate and return\n",
    "    # Manhattan distance from the agent to the target square.\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "    #Reset is called to initiate a new episode for an environment and has two parameters, seed and options. Seed initializes the\n",
    "    # random number generator to allow us to consistently generate the same environment when there are random variables involved.\n",
    "    # Options is a dict containing any additional parameters we might want to specify during the reset.\\\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "         #We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        #Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0,self.size,size=2,dtype=int)\n",
    "        #Sample random target locations until they do not coincide with the agent's starting location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location,self.agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0,self.size,size=2,dtype=int\n",
    "            )\n",
    "\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation,info\n",
    "    \n",
    "    def step(self, action):\n",
    "        #Map the action (element of {0,1,2,3}) to a direction on the map, using our helper dictionary\n",
    "        direction = self._action_to_direction[action]\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1)\n",
    "        \n",
    "        #We use `np.clip` to make sure we don't leave the grid bound\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        truncated = False\n",
    "        reward = 1 if terminated else 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "#Now that we've defined the environment in it's own class, we can register it with gymnasium under a particular namespace\n",
    "# which we can then call gym.make() on to instantiate this custom environment.\n",
    "gym.register(id=\"GridWorld-v0\",\n",
    "             entry_point = GridWorldEnv)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ed8a81-6d1d-4b41-b8f3-07e29169487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvChecker<GridWorldEnv<GridWorld-v0>>>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating the registered version of our custom environment using gym.make().\n",
    "gym.make(\"GridWorld-v0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003b9f1-821d-42df-9433-16e443e0c457",
   "metadata": {},
   "source": [
    "# Designing my own Custom Environment\n",
    "\n",
    "### Simplified single-player color-matching game\n",
    "\n",
    "Now I'll design my own custom environment in grid world. It will have elements of the ECG, but be designed to be solvable by a single player. In this environment, the agent will have to travel to a square whose color matches another given color. The squares will be inside of a 4x4 grid and the colors and starting position of the agent will be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75aa161b-f508-4193-91fb-cf0c5b293720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleColorGame(gym.Env):\n",
    "    #Initializes the Env, including observation space and action space. This one initializes the Observation space as a grid\n",
    "    # of boxes with colors assigned to them, and the action space as the movement of the agent along the grid.\n",
    "    def __init__(self,size=2):\n",
    "        #The size of one side of the square grid. It will be NxN squares in area, where N is self.size\n",
    "        self.size = size\n",
    "        self._num_colors = size**2\n",
    "        \n",
    "        #The agent location is stored inside of a local variable.\n",
    "        self._agent_location = np.array([-1,-1], dtype=np.int32)\n",
    "\n",
    "        #The colors of the boxes are also stored in a local variable. These colors are randomized on start-up. For this\n",
    "        # version of the game, I will substitute integer values for colors.\n",
    "        self._square_colors = np.arange(self._num_colors).reshape(size,size)\n",
    "\n",
    "        #The target color will be a random number between 1 and 4. This number will be initialized during the reset() method.\n",
    "        self._target_color = np.random.randint(0,self._num_colors)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent location\": gym.spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "                \"square colors\": gym.spaces.Box(0, self._num_colors-1, shape=(size,size), dtype=int),\n",
    "                \"target color\": gym.spaces.Discrete(self._num_colors)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        \n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "\n",
    "    #Helper method used to get the observation from the state, useful in reset and step methods. This version returns\n",
    "    # the properties of agent location, square colors, and the target color.\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "                \"agent location\": self._agent_location,\n",
    "                \"square colors\": self._square_colors,\n",
    "                \"target color\": self._target_color\n",
    "        }\n",
    "\n",
    "    #Helper method used to get auxiliary information from the state. Currently returns nothing.\n",
    "    def _get_info(self):\n",
    "        return None\n",
    "\n",
    "    #Reset the environment to an initial configuration. The initial state may involve some randomness, so the seed argument\n",
    "    # is used to guarantee an identical initial state whenever reset() is called with that seed. Options is a dict containing\n",
    "    # any additional parameters we might want to specify during the reset.\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \n",
    "        #Firstly, we will call this method to seed self.np_random with the seed argument if given.\n",
    "        super().reset(seed==seed)\n",
    "\n",
    "        #Now randomly generate a starting location for the agent using self.np_random. We generate an array of size two\n",
    "        # representing the agent's starting coordinates.\n",
    "        self._agent_location = self.np_random.integers(0,self.size,size=2)\n",
    "\n",
    "        #Generate a random permutation of the square colors, and reshape them into a sizeXsize grid.\n",
    "        self._square_colors = self.np_random.permutation(self._num_colors).reshape(self.size,self.size)\n",
    "\n",
    "        #Now we generate the target color, which is a random integer from 0 to self._num_colors inclusive.\n",
    "        self._target_color = self.np_random.integers(0,self._num_colors)\n",
    "\n",
    "        #Now we can return the observation and auxiliary info\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    #Takes an action as input and updates the state of the Env according to that Action. Step then returns an observation\n",
    "    # containing the new Env state, as well as some other additional variables and info.\n",
    "    def step(self, action):\n",
    "        #First, we convert our action to a direction.\n",
    "        direction = self._action_to_direction(action)\n",
    "        #Next, we clip the agent location at the boundary of the box, so the agent's coordinates are within 0 and self.size-1.\n",
    "        self._agent_location = np.clip(self._agent_location + direction,0,self.size-1)\n",
    "        \n",
    "        #Now we terminate the game and give the agent a reward if the square it's standing on is the target color.\n",
    "        terminated = (self._square_colors[tuple(self._agent_location)] == self._target_color)\n",
    "        truncated = False\n",
    "        reward = 1 if terminated else 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "#Now let's register this environment with a namespace and try calling gym.make on it later\n",
    "gym.register(id=\"SimpleColorGame-v0\",\n",
    "            entry_point = SimpleColorGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5927e473-ec41-40bc-9e02-61ff32cbe174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see if the registration worked! We ran into a few errors with incorrect method/variable names, but after fixing those it appears this\n",
    "# game was able to load!\n",
    "env = gym.make(\"SimpleColorGame-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66688d79-e52a-4bce-9625-801f53c93f94",
   "metadata": {},
   "source": [
    "## Learning in the Custom Env with Stable-Baselines\n",
    "\n",
    "Now that I have a custom environment defined, I can try unleashing the Stable-Baselines RL algorithms on it to see if I can train a model to succeed. Of course, I have no idea currently if the environment is working properly, so I'll have to find out through trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f84cf3a5-96ea-4a51-855e-4dd3c6a5c358",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must use `MultiInputPolicy` when working with dict observation space, not MlpPolicy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m A2C, PPO, DQN\n\u001b[1;32m----> 3\u001b[0m model_A2C \u001b[38;5;241m=\u001b[39m A2C(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,env, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:92\u001b[0m, in \u001b[0;36mA2C.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, rms_prop_eps, use_rms_prop, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, normalize_advantage, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     68\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m ):\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     93\u001b[0m         policy,\n\u001b[0;32m     94\u001b[0m         env,\n\u001b[0;32m     95\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     96\u001b[0m         n_steps\u001b[38;5;241m=\u001b[39mn_steps,\n\u001b[0;32m     97\u001b[0m         gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[0;32m     98\u001b[0m         gae_lambda\u001b[38;5;241m=\u001b[39mgae_lambda,\n\u001b[0;32m     99\u001b[0m         ent_coef\u001b[38;5;241m=\u001b[39ment_coef,\n\u001b[0;32m    100\u001b[0m         vf_coef\u001b[38;5;241m=\u001b[39mvf_coef,\n\u001b[0;32m    101\u001b[0m         max_grad_norm\u001b[38;5;241m=\u001b[39mmax_grad_norm,\n\u001b[0;32m    102\u001b[0m         use_sde\u001b[38;5;241m=\u001b[39muse_sde,\n\u001b[0;32m    103\u001b[0m         sde_sample_freq\u001b[38;5;241m=\u001b[39msde_sample_freq,\n\u001b[0;32m    104\u001b[0m         rollout_buffer_class\u001b[38;5;241m=\u001b[39mrollout_buffer_class,\n\u001b[0;32m    105\u001b[0m         rollout_buffer_kwargs\u001b[38;5;241m=\u001b[39mrollout_buffer_kwargs,\n\u001b[0;32m    106\u001b[0m         stats_window_size\u001b[38;5;241m=\u001b[39mstats_window_size,\n\u001b[0;32m    107\u001b[0m         tensorboard_log\u001b[38;5;241m=\u001b[39mtensorboard_log,\n\u001b[0;32m    108\u001b[0m         policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs,\n\u001b[0;32m    109\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    110\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    111\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    112\u001b[0m         _init_setup_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    113\u001b[0m         supported_action_spaces\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    114\u001b[0m             spaces\u001b[38;5;241m.\u001b[39mBox,\n\u001b[0;32m    115\u001b[0m             spaces\u001b[38;5;241m.\u001b[39mDiscrete,\n\u001b[0;32m    116\u001b[0m             spaces\u001b[38;5;241m.\u001b[39mMultiDiscrete,\n\u001b[0;32m    117\u001b[0m             spaces\u001b[38;5;241m.\u001b[39mMultiBinary,\n\u001b[0;32m    118\u001b[0m         ),\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_advantage \u001b[38;5;241m=\u001b[39m normalize_advantage\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Update optimizer inside the policy if we want to use RMSProp\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# (original implementation) rather than Adam\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:85\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     62\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m ):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     86\u001b[0m         policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[0;32m     87\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m     88\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     89\u001b[0m         policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs,\n\u001b[0;32m     90\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m     91\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     92\u001b[0m         use_sde\u001b[38;5;241m=\u001b[39muse_sde,\n\u001b[0;32m     93\u001b[0m         sde_sample_freq\u001b[38;5;241m=\u001b[39msde_sample_freq,\n\u001b[0;32m     94\u001b[0m         support_multi_env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m         monitor_wrapper\u001b[38;5;241m=\u001b[39mmonitor_wrapper,\n\u001b[0;32m     96\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m     97\u001b[0m         stats_window_size\u001b[38;5;241m=\u001b[39mstats_window_size,\n\u001b[0;32m     98\u001b[0m         tensorboard_log\u001b[38;5;241m=\u001b[39mtensorboard_log,\n\u001b[0;32m     99\u001b[0m         supported_action_spaces\u001b[38;5;241m=\u001b[39msupported_action_spaces,\n\u001b[0;32m    100\u001b[0m     )\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m=\u001b[39m n_steps\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:192\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Catch common mistake: using MlpPolicy/CnnPolicy instead of MultiInputPolicy\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m policy \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, spaces\u001b[38;5;241m.\u001b[39mDict):\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must use `MultiInputPolicy` when working with dict observation space, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneralized State-Dependent Exploration (gSDE) can only be used with continuous actions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: You must use `MultiInputPolicy` when working with dict observation space, not MlpPolicy"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C, PPO, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model_A2C = A2C(\"MultiInputPolicy\", env, verbose = 0)\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes = 100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward.2f}\")\n",
    "      \n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, env, n_eval_episodes = 100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845b3f4-a0d0-4db8-b00a-ab144eb26586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
