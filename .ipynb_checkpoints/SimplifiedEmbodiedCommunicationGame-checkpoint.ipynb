{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d955a43-b147-44fa-b32c-949c017f3d39",
   "metadata": {},
   "source": [
    "## Simpler ECG with Lagging State Memory\n",
    "\n",
    "In order to allow the RL models to learn communication, I'm modifying the ECG in two ways:\n",
    "1. I'm decreasing the size of the game boards and the number of colors\n",
    "2. I'm adding a 'lagging memory' to the Observation Space of each agent, consisting of the last 10 moves of the other agent, which should allow them to 'see' and react to any attempts by the other agent to communicate (which will necessarily occur over multiple in-game timesteps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6aa90e0-e146-4d7d-af9e-4a1390477ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Imports\n",
    "import os\n",
    "from copy import copy\n",
    "import functools\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gymnasium Imports\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete\n",
    "from gymnasium.utils import seeding\n",
    "\n",
    "#PettingZoo and Supersuit Imports\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.test import parallel_api_test\n",
    "import supersuit as ss\n",
    "\n",
    "# SB3 Imports\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor, SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "#My Custom Functions\n",
    "from functions import plot_results, plot_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "295208cb-ed68-44f9-9f8a-f8a074bd5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbodiedCommunicationGame(ParallelEnv):\n",
    "    \"\"\"\n",
    "    My second attempt at creating emergent communication in the RL models. This game differs from the original in 2 ways:\n",
    "    1. The grids are smaller with less possible colors\n",
    "    2. The agents are fed the previous agents' previous 10 locations in addition to their current location. This allows the models to interpret\n",
    "    behavior over time as communiction.\n",
    "    \"\"\"\n",
    "    metadata = { \"render_modes\": [],\n",
    "        \"name\":\"EmbodiedCommunicationGame-v0\"}\n",
    "    \n",
    "    def __init__(self, step_limit = 100):\n",
    "        \"\"\"\n",
    "        Variables to instantiate:\n",
    "        -2 2x1 Grids \n",
    "        -2 Agent locations (int)\n",
    "        -Color List (3 colors)\n",
    "        -Step Count\n",
    "        -Step Limit\n",
    "        -Possible Agents (agents)?\n",
    "        -Lagging Memory (dict{a: np.array(1,10)})\n",
    "        We will not initialize the agent locations or grid color patterns here, that will occur in self.reset().\n",
    "        We instantiate the variables here to keep better track of them.\n",
    "        \"\"\"\n",
    "\n",
    "        #public variables\n",
    "        self.step_limit = step_limit\n",
    "        self.possible_agents = [\"agent1\", \"agent2\"]\n",
    "        self.render_mode = None\n",
    "        self.spec = None\n",
    "        \n",
    "        #private variables\n",
    "        self._step_count = 0\n",
    "\n",
    "        #Grids, agent coords, and agents committed. To make step() easier, I'm going to put each of these into it's own\n",
    "        # dict, with keys corresponding to agent names.\n",
    "        self._color_grids = {a : np.zeros((2,)) for a in self.possible_agents}\n",
    "\n",
    "        self._agent_coords = {a : -1 for a in self.possible_agents}\n",
    "\n",
    "        #Track whether agents are committed.\n",
    "        self._agents_committed = {a : False for a in self.possible_agents}\n",
    "        \n",
    "        #Lagging Memory for agent locations\n",
    "        self._lagging_memory = {a : np.zeros((10,)) for a in self.possible_agents}\n",
    "        \n",
    "        #Dictionary to map action space to a direction on the Gridworld.\n",
    "        self._action_to_direction = {\n",
    "            0: -1, #Left,\n",
    "            1: 1, #Right,\n",
    "            2: 0, #Chill,\n",
    "            3: 0, #Commit\n",
    "        }\n",
    "        \n",
    "        #Color list\n",
    "        self._colors = [[0,1],[0,2],[1,2]]\n",
    "            \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        initialize the Env for play.\n",
    "        Variables to initialize:\n",
    "        -Agent locations\n",
    "        -Color arrangements for each 2x2 grid\n",
    "        -step-count\n",
    "        -Agent committed flag\n",
    "        \"\"\"\n",
    "        #call super().reset() with seed to set self.np_random with seed.\n",
    "        self._seed(seed)\n",
    "\n",
    "        #Reset flags\n",
    "        self._step_count = 0\n",
    "\n",
    "        #Instantiate self.agents to be a copy (by value) of self.possible_agents.\n",
    "        self.agents = copy(self.possible_agents)\n",
    "        \n",
    "        #generate color grids by first generating the grid selections for each agent\n",
    "        grid_choices = self.np_random.choice(3, size=(2,),replace=False)\n",
    "\n",
    "        #Iteratively generate agent data, including grid color, agent location, and agent committed.\n",
    "        for idx, a in enumerate(self.agents):\n",
    "            self._color_grids[a] = self.np_random.permutation(grid_choices[idx])\n",
    "            self._agent_coords[a] = self.np_random.integers(2)\n",
    "            self._agents_committed[a] = False\n",
    "            self._lagging_memory[a].fill(-1)\n",
    "        \n",
    "        #now to return obs and infos\n",
    "        obs = self._get_obs()\n",
    "        infos = self._get_infos()\n",
    "        # print(\"obs: \",obs,\"infos: \",infos, sep=\"\\n\\n\")\n",
    "        \n",
    "        return obs, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        update the Env according to the actions of each agent.\n",
    "        returns:\n",
    "        obs{}, rewards{}, terminateds{}, truncateds{}, infos{}\n",
    "        \"\"\"\n",
    "        for (agent, action) in actions.items():\n",
    "            #roll the lagging memory\n",
    "            self._lagging_memory[agent] = np.roll(self._lagging_memory[agent],1)\n",
    "            \n",
    "            #update the agent committed status first.\n",
    "            if action == 3:\n",
    "                self._agents_committed[agent] = True\n",
    "                \n",
    "            #Update agent's coordinates only if agent not committed. Checking action != 4 for redundancy.\n",
    "            if (self._agents_committed[agent] == False and action != 3):\n",
    "                direction = self._action_to_direction[action]\n",
    "                self._agent_coords[agent] = np.clip(self._agent_coords[agent] + direction, 0, 1)\n",
    "                self._lagging_memory[agent][0] = self._agent_coords[agent]\n",
    "\n",
    "        self._step_count += 1\n",
    "        \n",
    "        #Now calculate the return values.\n",
    "        obs = self._get_obs()\n",
    "        rewards, terminateds, truncateds = self._get_rewards_terminateds_truncateds()\n",
    "        infos = self._get_infos()\n",
    "\n",
    "        if all(terminateds.values()) or all(truncateds.values()):\n",
    "            self.agents = []\n",
    "\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _seed(self, seed = None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        #I tried getting fancy and generating this with a loop, but it was causing problems, so for now I'm just going\n",
    "        # to make it by hand.\n",
    "        a1 = self.agents[0]\n",
    "        a2 = self.agents[1]\n",
    "        obs = {\n",
    "            a1 : {\n",
    "                \"myCoords\" : self._agent_coords[a1],\n",
    "                \"theirCoords\" : self._lagging_memory[a2],\n",
    "                \"myColors\" : self._color_grids[a1],\n",
    "            },\n",
    "            a2 : {\n",
    "                \"myCoords\" : self._agent_coords[a2],\n",
    "                \"theirCoords\" : self._lagging_memory[a1],\n",
    "                \"myColors\" : self._color_grids[a2],\n",
    "            },\n",
    "        }\n",
    "                     \n",
    "        return obs\n",
    "\n",
    "    def _get_rewards_terminateds_truncateds(self):\n",
    "        \"\"\"\n",
    "        this function will return the rewards, terminateds, and truncateds state.\n",
    "        There is specific logic relating rewards to truncation and termination.\n",
    "        \"\"\"   \n",
    "        #we're basically building up a long boolean check to see if the agents have both committed to squares of the\n",
    "        # same color. For that reason, I'm going to shorten some of these variable names to make the boolean statement\n",
    "        # more readable.\n",
    "        a1 = self.agents[0]\n",
    "        a2 = self.agents[1]\n",
    "        grid1 = self._color_grids[a1]\n",
    "        grid2 = self._color_grids[a2]\n",
    "        a1_coords = self._agent_coords[a1]\n",
    "        a2_coords = self._agent_coords[a2]\n",
    "\n",
    "        #Common/base case for terminated/reward is Terminated = False and Reward = 0. I want to also punish agents for\n",
    "        # not committing to speed up the process of committing at some point.\n",
    "        rewards = {a: (-1 if self._step_count > self.step_limit else 0) for a in self.agents}\n",
    "        terminateds = {a: False for a in self.agents}\n",
    "        truncateds = {a : self._step_count > self.step_limit for a in self.agents}\n",
    "\n",
    "        #If both of the agents have committed, Terminated = True.\n",
    "        if (self._agents_committed[a1] == True and self._agents_committed[a2] == True):\n",
    "            terminateds = {a : True for a in self.agents}\n",
    "            \n",
    "            #If the color of a1's square matches the color of a2's square, reward = 1. Noteably, we only reward\n",
    "            # the agents for having matching square colors if both agents have chosen to commit to their squares.\n",
    "            if (grid1[a1_coords] == grid2[a2_coords]):\n",
    "                rewards = {a : 1 for a in self.agents}\n",
    "        \n",
    "        return rewards, terminateds, truncateds\n",
    "\n",
    "    #TODO: Learn more about what I can use info for. this might be a way to give the agents some form of memory.\n",
    "    def _get_infos(self):\n",
    "        return {a : {} for a in self.agents}\n",
    "\n",
    "    #Farama recommends using memo-ized functions for returning action_space and observation_space rather than public\n",
    "    # or private class variables.\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self,agent):\n",
    "        return gym.spaces.Dict({\n",
    "            \"myCoords\": gym.spaces.Box(0,2, shape = (2,), dtype=int),\n",
    "            \"theirCoords\": gym.spaces.Box(0,2, shape = (10,), dtype=int),\n",
    "            \"myColors\": gym.spaces.Box(0,3, shape = (2,), dtype=int),\n",
    "        })\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self,agent):\n",
    "        return Discrete(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dbaa229-f2d5-4280-a28a-c10464135083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  0. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  0. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  0.  0. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  0.  0. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  0.  0.  0. -1. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  0.  0.  0. -1. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  0.  0.  0.  0. -1. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  0.  0.  0.  0.  0. -1. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  1.  0.  0.  0.  0.  0. -1. -1. -1.]\n",
      "(10,)\n",
      "[-1.  1.  1.  0.  0.  0.  0.  0. -1. -1.]\n",
      "(10,)\n",
      "[-1.  1.  1.  1.  0.  0.  0.  0.  0. -1.]\n",
      "(10,)\n",
      "[-1.  1.  1.  1.  1.  0.  0.  0.  0.  0.]\n",
      "(10,)\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "(10,)\n",
      "[0. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "(10,)\n",
      "[0. 0. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "(10,)\n",
      "[0. 0. 0. 1. 1. 1. 1. 1. 1. 0.]\n",
      "(10,)\n",
      "[0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m SimpleEmbodiedCommunicationGame()\n\u001b[1;32m----> 2\u001b[0m parallel_api_test(env, num_cycles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#One weird error I encountered was that it told me\u001b[39;00m\n\u001b[0;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m ss\u001b[38;5;241m.\u001b[39mpettingzoo_env_to_vec_env_v1(env)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PythonRL\\Lib\\site-packages\\pettingzoo\\test\\parallel_test.py:68\u001b[0m, in \u001b[0;36mparallel_api_test\u001b[1;34m(par_env, num_cycles)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_cycles):\n\u001b[0;32m     60\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m         agent: sample_action(par_env, obs, agent)\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m par_env\u001b[38;5;241m.\u001b[39magents\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         )\n\u001b[0;32m     67\u001b[0m     }\n\u001b[1;32m---> 68\u001b[0m     obs, rew, terminated, truncated, info \u001b[38;5;241m=\u001b[39m par_env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m par_env\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m agent \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m has_finished, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent cannot be revived once dead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[31], line 118\u001b[0m, in \u001b[0;36mSimpleEmbodiedCommunicationGame.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m#Now calculate the return values.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs()\n\u001b[1;32m--> 118\u001b[0m rewards, terminateds, truncateds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rewards_terminateds_truncateds()\n\u001b[0;32m    119\u001b[0m infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_infos()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(terminateds\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(truncateds\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "Cell \u001b[1;32mIn[31], line 176\u001b[0m, in \u001b[0;36mSimpleEmbodiedCommunicationGame._get_rewards_terminateds_truncateds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m     terminateds \u001b[38;5;241m=\u001b[39m {a : \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents}\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m#If the color of a1's square matches the color of a2's square, reward = 1. Noteably, we only reward\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# the agents for having matching square colors if both agents have chosen to commit to their squares.\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (grid1[a1_coords] \u001b[38;5;241m==\u001b[39m grid2[a2_coords]):\n\u001b[0;32m    177\u001b[0m         rewards \u001b[38;5;241m=\u001b[39m {a : \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents}\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rewards, terminateds, truncateds\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "env = SimpleEmbodiedCommunicationGame()\n",
    "parallel_api_test(env, num_cycles=1000)\n",
    "\n",
    "#One weird error I encountered was that it told me\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, 10, base_class = \"stable_baselines3\")\n",
    "#Wrapping the Env in the SB3 Monitor() Wrapper requires the Env to have a .spec attribute, which the SuperSuit conversion\n",
    "# never added. This attribute can be None. I'm setting it manually so I can use SB3's Monitor Wrapper.\n",
    "# env.spec = None\n",
    "env = VecMonitor(env, mPPO_dir)\n",
    "mPPO = RecurrentPPO(\n",
    "    \"MultiInputLstmPolicy\",\n",
    "    env,\n",
    "    verbose = 0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
