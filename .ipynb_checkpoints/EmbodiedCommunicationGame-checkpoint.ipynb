{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b63349-155d-4a79-aa5b-dc32d37edf28",
   "metadata": {},
   "source": [
    "### Alright, Time for the Grand Finale.\n",
    "\n",
    "We've gotten the models to learn the Simple Color Game in a 2x2 and a 3x3 grid, so now it's time to build up to the full 2-player Embodied Communication Game. The first step I'm going to take is to add a 5th action, \"Stop\" to the models, which they will have to take on the target square to recieve a reward.\n",
    "\n",
    "I'll also be training the models here using a curriculum approach, where the reward function changes over time. I'll first allow them to recieve a reward whenever they touch the target square, then I'll give them an increased reward when they stop on the target square. Finally, I'll remove all other rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530e5857-5126-428c-a865-1ab59fba548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "# GYMNASIUM IMPORTS\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# SB3 ALGORITHM IMPORTS\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from functions import plot_results, plot_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a75068-9bd0-4274-b5e2-9e9b90db1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be the v1 of the game, with a stop function included.\n",
    "class SimpleColorGame(gym.Env):\n",
    "    # Initializes the Env, including observation space and action space. This one initializes the Observation space as a grid\n",
    "    # of boxes with colors assigned to them, and the action space as the movement of the agent along the grid.\n",
    "    def __init__(self, size=2, step_limit=200):\n",
    "        # The size of one side of the square grid. It will be NxN squares in area, where N is self._size\n",
    "        self._size = size\n",
    "        self._num_colors = size**2\n",
    "\n",
    "        # This is a time limit on the number of steps the agent is allowed to take in the game. This is necessary to\n",
    "        # prevent the game from running forever if the agent's policy prevents it from moving or reaching the target.\n",
    "        self._step_limit = step_limit\n",
    "        # Integer to keep track of the number of steps taken in a particular iteration of the game\n",
    "        self._step_count = 0\n",
    "\n",
    "        #Boolean flag for STOP action. When the agent stops, this becomes true.\n",
    "        self._stopped = False;\n",
    "\n",
    "        # The agent location is stored inside of a local variable.\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # The colors of the boxes are also stored in a local variable. These colors are randomized on start-up. For this\n",
    "        # version of the game, I will substitute integer values for colors.\n",
    "        self._square_colors = np.arange(self._num_colors).reshape(size, size)\n",
    "\n",
    "        # The target color will be a random number between 1 and 4. This number will be initialized during the reset() method.\n",
    "        self._target_color = np.random.randint(0, self._num_colors)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent location\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"square colors\": gym.spaces.Box(\n",
    "                    0, self._num_colors - 1, shape=(size, size), dtype=int\n",
    "                ),\n",
    "                \"target color\": gym.spaces.Discrete(self._num_colors),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 5 actions, corresponding to \"right\", \"up\", \"left\", \"down\", and \"stop\"\n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "            4: np.array([0,0]), #stop\n",
    "        }\n",
    "\n",
    "    # Helper method used to get the observation from the state, useful in reset and step methods. This version returns\n",
    "    # the properties of agent location, square colors, and the target color.\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent location\": self._agent_location,\n",
    "            \"square colors\": self._square_colors,\n",
    "            \"target color\": self._target_color,\n",
    "        }\n",
    "\n",
    "    # Helper method used to get auxiliary information from the state. Currently returns nothing.\n",
    "    def _get_info(self):\n",
    "        info = {\"info\": None}\n",
    "        return info\n",
    "\n",
    "    # Helper method for calculating the reward from the state. This will be useful as I can override it in child classes.\n",
    "    def _get_reward(self):\n",
    "        reward = 0\n",
    "        if (self._square_colors[tuple(self._agent_location)] == self._target_color):\n",
    "            reward = 2 if (self._stopped == True) else 1\n",
    "        return reward\n",
    "\n",
    "    # Reset the environment to an initial configuration. The initial state may involve some randomness, so the seed argument\n",
    "    # is used to guarantee an identical initial state whenever reset() is called with that seed. Options is a dict containing\n",
    "    # any additional parameters we might want to specify during the reset.\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "\n",
    "        # Firstly, we will call this method to seed self.np_random with the seed argument if given.\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset the step count to 0 for the new iteration of the game\n",
    "        self._step_count = 0\n",
    "\n",
    "        # Now randomly generate a starting location for the agent using self.np_random. We generate an array of size two\n",
    "        # representing the agent's starting coordinates.\n",
    "        self._agent_location = self.np_random.integers(0, self._size, size=2)\n",
    "\n",
    "        # Generate a random permutation of the square colors, and reshape them into a sizeXsize grid.\n",
    "        self._square_colors = self.np_random.permutation(self._num_colors).reshape(\n",
    "            self._size, self._size\n",
    "        )\n",
    "\n",
    "        # Now we generate the target color, which is a random integer from 0 to self._num_colors inclusive.\n",
    "        self._target_color = self.np_random.integers(0, self._num_colors)\n",
    "\n",
    "        # Now we can return the observation and auxiliary info\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    # Takes an action as input and updates the state of the Env according to that Action. Step then returns an observation\n",
    "    # containing the new Env state, as well as some other additional variables and info.\n",
    "    def step(self, action):\n",
    "        # First, iterate the step count by one\n",
    "        self._step_count += 1\n",
    "\n",
    "        # Next, we convert our action to a direction.\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        #Finally, we check for the STOP action.\n",
    "        if action == 4:\n",
    "            self._stopped = True\n",
    "\n",
    "        # Then we add the direction coordinates to the agend coordinates to get the new agent location. We must clip the\n",
    "        # agent location at the Box boundary, so the agent's coordinates are within 0 and self._size-1.\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self._size - 1\n",
    "        )\n",
    "\n",
    "        # Now we terminate the game and give the agent a reward if the square it's standing on is the target color.\n",
    "        terminated = (\n",
    "            self._square_colors[tuple(self._agent_location)] == self._target_color\n",
    "        )\n",
    "\n",
    "        # We also truncate the game if self._step_count > self._step_limit.\n",
    "        truncated = self._step_count > self._step_limit\n",
    "\n",
    "        # Reward is 1 if we are on the target color square, otherwise 0\n",
    "        reward = self._get_reward()\n",
    "\n",
    "        # Finally, use the helper functions to generate Obs and Info.\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# Now let's register this environment with a namespace and try calling gym.make on it later\n",
    "gym.register(id=\"SimpleColorGame-v1\", entry_point=SimpleColorGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c1380-6dc5-47cb-8038-cbf468cff3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be the v1 of the game, with a stop function included.\n",
    "class StopOnColorGame(gym.Env):\n",
    "    # Initializes the Env, including observation space and action space. This one initializes the Observation space as a grid\n",
    "    # of boxes with colors assigned to them, and the action space as the movement of the agent along the grid.\n",
    "    def __init__(self, size=2, step_limit=200):\n",
    "        self._size = size\n",
    "        self._num_colors = size**2\n",
    "        self._step_limit = step_limit\n",
    "        self._step_count = 0\n",
    "        self._stopped = False;\n",
    "\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._square_colors = np.arange(self._num_colors).reshape(size, size)\n",
    "        self._target_color = np.random.randint(0, self._num_colors)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent location\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"square colors\": gym.spaces.Box(\n",
    "                    0, self._num_colors - 1, shape=(size, size), dtype=int\n",
    "                ),\n",
    "                \"target color\": gym.spaces.Discrete(self._num_colors),\n",
    "            }\n",
    "        )\n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "            4: np.array([0,0]), #stop\n",
    "        }\n",
    "\n",
    "    # Helper method used to get the observation from the state, useful in reset and step methods. This version returns\n",
    "    # the properties of agent location, square colors, and the target color.\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent location\": self._agent_location,\n",
    "            \"square colors\": self._square_colors,\n",
    "            \"target color\": self._target_color,\n",
    "        }\n",
    "\n",
    "    # Helper method used to get auxiliary information from the state. Currently returns nothing.\n",
    "    def _get_info(self):\n",
    "        info = {\"info\": None}\n",
    "        return info\n",
    "\n",
    "    # Helper method for calculating the reward from the state. This will be useful as I can override it in child classes.\n",
    "    def _get_reward(self):\n",
    "        reward = 1 if (self._stopped == True and self._square_colors[tuple(self._agent_location)] == self._target_color) else 0\n",
    "        return reward\n",
    "\n",
    "    def _get_terminated(self):\n",
    "        return self._stopped\n",
    "\n",
    "    def _get_truncated(self):\n",
    "        return self._step_count > self._step_limit;\n",
    "    # Reset the environment to an initial configuration. The initial state may involve some randomness, so the seed argument\n",
    "    # is used to guarantee an identical initial state whenever reset() is called with that seed. Options is a dict containing\n",
    "    # any additional parameters we might want to specify during the reset.\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "\n",
    "        # Firstly, we will call this method to seed self.np_random with the seed argument if given.\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset the step count to 0 for the new iteration of the game\n",
    "        self._step_count = 0\n",
    "\n",
    "        # Reset the _stopped flag\n",
    "        self._stopped = False\n",
    "\n",
    "        # Now randomly generate a starting location for the agent using self.np_random. We generate an array of size two\n",
    "        # representing the agent's starting coordinates.\n",
    "        self._agent_location = self.np_random.integers(0, self._size, size=2)\n",
    "\n",
    "        # Generate a random permutation of the square colors, and reshape them into a sizeXsize grid.\n",
    "        self._square_colors = self.np_random.permutation(self._num_colors).reshape(\n",
    "            self._size, self._size\n",
    "        )\n",
    "\n",
    "        # Now we generate the target color, which is a random integer from 0 to self._num_colors inclusive.\n",
    "        self._target_color = self.np_random.integers(0, self._num_colors)\n",
    "\n",
    "        # Now we can return the observation and auxiliary info\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    # Takes an action as input and updates the state of the Env according to that Action. Step then returns an observation\n",
    "    # containing the new Env state, as well as some other additional variables and info.\n",
    "    def step(self, action):\n",
    "        # First, iterate the step count by one\n",
    "        self._step_count += 1\n",
    "\n",
    "        # Next, we convert our action to a direction.\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        #Finally, we check for the STOP action.\n",
    "        if action == 4:\n",
    "            self._stopped = True\n",
    "\n",
    "        # Then we add the direction coordinates to the agend coordinates to get the new agent location. We must clip the\n",
    "        # agent location at the Box boundary, so the agent's coordinates are within 0 and self._size-1.\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self._size - 1\n",
    "        )\n",
    "        \n",
    "        terminated = self._get_terminated()\n",
    "        truncated = self._get_truncated()\n",
    "        reward = self._get_reward()\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# Now let's register this environment with a namespace and try calling gym.make on it later\n",
    "gym.register(id=\"StopOnColorGame-v1\", entry_point=StopOnColorGame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
