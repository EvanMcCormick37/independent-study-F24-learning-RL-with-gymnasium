{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77604744-d0d8-48e7-996e-e8515203ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Imports\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, PPO, DQN\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import pygame\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32b130-134d-4f9f-86e5-d0c73b03732c",
   "metadata": {},
   "source": [
    "# Goals for this Stage of Research\n",
    "* Create and render three different Gymnasium environments\n",
    "* Train three different StableBaseline3 RL models on each environment\n",
    "* Measure performance of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134d340-b02e-42f8-bb27-a9c5603a4ebd",
   "metadata": {},
   "source": [
    "### 1. Taxi-V3\n",
    "Here we're running the Taxi-v3 environment in Gymnasium with StableBaseline3's VecEnv (Vectorized Environment) which allows for simultaneous training on multiple instances of a Gymnasium environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a260a72-7785-41d6-b988-e9361af7dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = make_vec_env(\"Taxi-v3\",n_envs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a1fcc-ce68-4a08-9c5e-519e8f59cc17",
   "metadata": {},
   "source": [
    "#### Now, we will train 3 models and look at their performance increases after training, using the **evaluate_policy** function from Stable-Baeslines3.\n",
    "1. Proximal Policy Optimization (PPO): A reinforcement learning algorithm in which changes in the model weights are clipped to prevent drastic changes in policy which may result in a sudden drop in performance.\n",
    "2. Actor Advantage Critic (A2C): This algorithm uses two function approximators (neural networks) -- One to determine the policy, and one to estimate how good the action chosen is (the action's *Q-value*). The policy decider (Actor) is actually dependent not on the ultimate outcome of its policy, but on the opinion of the Q-value estimator (Critic). This model can actually speed up the learning process in some environments, though it isn't obvious to me why that is. Perhaps it's because the critic offers a more efficient way to estimate the q-value of a given action than averaging over the course of an entire episode of the game, especially when the rewards provided by the environment are sparse.\n",
    "3. Deep Q-Network (DQN): The fundamental model when applying deep-learning to a reinforcement learning algorithm. Older RL algorithms used a *Q-table(State,Action->Q)* where Q is the expected reward of a given action in a given state. A DQN model replaces the Q table with a function approximator (i.e. a neural network) for the function *(State,Action)->Q*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4203634-14a9-40bd-aa8a-bd49f41739ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -1258.49 +/- 882.40\n",
      "Mean Reward: -200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "#PPO MODEL\n",
    "\n",
    "model1 = PPO(MlpPolicy, vec_env, verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d240e367-1788-4f1a-83b9-5ebd41482f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -1294.40 +/- 875.09\n",
      "mean reward: -200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "#A2C MODEL\n",
    "\n",
    "model2 = A2C(\"MlpPolicy\", vec_env, verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model2.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ade119c-12de-4a4b-af69-608b4a7b6e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -2000.00 +/- 0.00\n",
      "mean reward: -200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "#DQN MODEL\n",
    "\n",
    "model3 = DQN(\"MlpPolicy\", vec_env, verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42b606-3d80-498f-82a2-029c96420aad",
   "metadata": {},
   "source": [
    "### All three models improved in the Taxi-v0 Env!\n",
    "\n",
    "Interestingly, the A2C and PPO models out-performed the DQN model initially, but all three converged on what appears to be the maximum score of -200 after only 10000 training timesteps.\n",
    "\n",
    "All three algorithms are viable in this simple Env.\n",
    "\n",
    "### Cartpole-v1\n",
    "\n",
    "Let's now test three models (A2C, PPO, and DQN) in Cartpole-v1 to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6a99f7-a3ed-463a-9d73-126f8d130729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 9.36 +/- 0.77\n",
      "mean reward: 120.66 +/- 37.59\n"
     ]
    }
   ],
   "source": [
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "model1 = DQN(\"MlpPolicy\", vec_env, verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.learn(total_timesteps=100000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c083ac61-26ec-47cd-ac63-d7a795bd651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 9.18 +/- 0.70\n",
      "mean reward: 500.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "model2 = A2C(\"MlpPolicy\", vec_env, verbose=0)\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model2.learn(total_timesteps=100000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a06fec1-642c-439e-9d46-b2f8edb82006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 30.00 +/- 6.25\n",
      "mean reward: 118.85 +/- 48.78\n"
     ]
    }
   ],
   "source": [
    "model3 = PPO(\"MlpPolicy\", vec_env, verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.learn(total_timesteps=100000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c54502-8576-42bf-bef9-05fad8c63d57",
   "metadata": {},
   "source": [
    "### A2C Outperformed DQN and PPO!\n",
    "\n",
    "Interestingly, the A2C model converged on the optimal solution the fastest, and is now able to keep the pole up indefinitely. PPO and DQN are slower to progress, possibly because PPO is clipped, but more likely because A2C's second function approximator sped up its learning process!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336e4dd-2e6f-4bcb-aeff-74f9a66eeb64",
   "metadata": {},
   "source": [
    "### FrozenLake-v1\n",
    "This is another simple environment, which includes an element of randomness in how the model interacts with its' environment. In this environment, the agent attempts to reach a finish goal by moving across a slippery ice-field, trying to avoid holes in the ice. Due to the 'slipperiness' of the ice, the agent has a small chance of randomly moving in a direction other than the one chosen by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "197da738-b8bf-4daa-9ebe-4efa6bee5232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_env = make_vec_env('FrozenLake-v1', n_envs=4)\n",
    "vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b474642-90c7-4f4a-a9f6-6d8a0134c637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 0.00 +/- 0.00\n",
      "mean reward: 0.54 +/- 0.50\n"
     ]
    }
   ],
   "source": [
    "model1 = PPO('MlpPolicy',vec_env,verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model1.learn(total_timesteps=100000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model1, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "887a7499-8af2-4196-8f01-bcc3b34b4574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 0.00 +/- 0.00\n",
      "mean reward: 0.74 +/- 0.44\n"
     ]
    }
   ],
   "source": [
    "model2 = A2C('MlpPolicy',vec_env,verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model2.learn(total_timesteps=100000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model2, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a870b93-b057-41eb-aa0f-65cb8b23b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 0.00 +/- 0.00\n",
      "mean reward: 0.52 +/- 0.50\n"
     ]
    }
   ],
   "source": [
    "model3 = DQN('MlpPolicy',vec_env,verbose=0)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "model3.learn(total_timesteps=100000)\n",
    "\n",
    "mean_reward,std_reward = evaluate_policy(model3, vec_env, n_eval_episodes=100)\n",
    "print(f\"mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3463cb7d-11db-4319-a8c2-1b4d1fe783c8",
   "metadata": {},
   "source": [
    "Again, it seems the A2C model out-performed the other two models. This might imply that A2C is better in general, or just better at training in Vectorized environments. Jury's still out on this.\n",
    "\n",
    "Edit: Future Evan here, interestingly, PPO consistently out-performed A2C in the Embodied Communication Game. However, these models are also profoundly inconsistent, and running a single test per **Model**x**Environment** is likely an unreliable metric of model performance overall. I'm looking into a way to combin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
